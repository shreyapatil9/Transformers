{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 7610 Special Topics in NLP, Lecture 6 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 16 June 2022, with material from the torch and tensorflow people, and Jay Alammar</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Architecture for Tensorflow 2.x\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/bumble-bee.jpg\" width=300 />\n",
    "</left>\n",
    "With recurrent neural networks, you input a tensor and you get a matrix out.\n",
    "\n",
    "In a **dense** (fully connected) neural network, you have one output per input. You have an image and you have a label, for instance. There is no way you can input image after image on Neural Networks and get an output based on all of them. The nature of dense neural networks make them unable to process **sequential data**.\n",
    "\n",
    "On the other hand, RNNs work very well with sequential data. They have a mechanism for **remembering** the previous inputs and producing an output based on all of the inputs. This makes them well-suited for sequential type of data such as text, audio, video or any time-series data.\n",
    "\n",
    "However, owing to their iterative nature, it takes quite a long time for information to propagate (linear in the length of the sequence) through RNNs, making it difficult to exploit the merits of concurrent hardware such as GPUs. \n",
    "\n",
    "CNNs are immune to this problem as each kernel application is performed in isolation (within the same layer). Unfortunately this comes at the cost of only being able to account for fixed-sized contexts. \n",
    "\n",
    "Dense fully-connected layers do not account for sequential information and have yet to prove their suitability for these tasks.\n",
    "\n",
    "Released in late 2017, [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) is the 3rd big revolution in deep learning community and is now considered as a go-to method for sequence transduction (NLP) tasks. \n",
    "\n",
    "The so-called **Transformer** leverages **fully-connected** networks with an enhanced **attention** mechanism. No more RNNs!\n",
    "\n",
    "The core idea behind the Transformer model is an expansion of the mechanism of **self-attention** we already studied: \n",
    "\n",
    ">**Definition**: Self-attention is the ability to attend to different positions of the input sequence to compute a representation of that sequence. \n",
    "\n",
    "Transformer creates **stacks** of ((self-attention layers, ***in both the encoder and decoder***, as well as ***cross-attention between the 2*** (what we called semi-Attention and already studied). That's a lot of Attention!\n",
    "\n",
    "The Transformer is the first transduction model relying entirely on **Attention** to compute representations of its input and output without using sequence aligned RNNs or CNNs. Read the google [post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) introducing it.\n",
    "\n",
    "Google says The Transformer model:\n",
    "- Yields higher translation quality\n",
    "- Requires less computation to train\n",
    "- Is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude\n",
    "\n",
    "Let's run experiments to see if that is true!\n",
    "\n",
    "A transformer model handles variable-sized input using stacks of self-attention layers instead of RNNs or CNNs. Advantages are:\n",
    "\n",
    "- Layer outputs can be calculated in parallel, instead of in a series like an RNN (even though we batch the sequences)\n",
    "- Distant items can affect each other's output without passing through many RNN-steps or convolution layers, so it can effectively learn long-range dependencies\n",
    "\n",
    "Downsides are:\n",
    "\n",
    "- For a time-series, the output for a time-step is calculated from the **entire history** instead of only the inputs and current hidden-state. This may be less efficient. But google says that's not the case.\n",
    "- If the input does have a temporal/spatial relationship, as with text, some positional encoding must be added or the model will effectively see a **bag of words**. Transformer does include that.\n",
    "- It's more complicated than seq2seq with attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. s2s with Attention and Transformer Attention\n",
    "\n",
    "## s2s without Attention\n",
    "In transducing a sequence $x = (x_1, \\ldots, x_T)$ of variable length $T$ into another sequence $y = (y_1, \\ldots, y_{T'})$ of length T’, the first sequence $x$ is first consumed by an encoder RNN which yields a final state $h_T$ using the following equation:\n",
    "\n",
    "$$h_{t+1} = σ (W_E h_t +Q_E x_{t+1} ) ∈ R^{d_H}$$\n",
    "\n",
    "where $h_0$  is typically zero initialized and $σ()$ denotes some suitable non-linearity. \n",
    "\n",
    "A decoder RNN takes the $d_H$ dimensional output $h_T$  of the encoder as an initial condition and produces a variably-sized sequence:\n",
    "\n",
    "$$g_{t+1} = \\sigma(W_D g_{t} + Q_D y_t) \\in \\mathbb{R}^{d_G}$$\n",
    " \n",
    "$$y_{t+1} = \\textrm{softmax}(W_Y g_{t+1}) \\in \\mathbb{R}^{d_V}$$\n",
    "\n",
    "where $y_0 = h_T$  and $g_0 = \\mathbf{0}$. \n",
    "\n",
    "Notice that the last generated output token $y$ is used as an input in the next step. In practice, an end token (`<end>`) often serves as a termination symbol so that the model doesn’t produce sequences of infinite length in prediction mode.\n",
    "\n",
    "The above model still has one *profound* shortcoming - it needs to ***pack*** all of the information of the input sequence into a single vector $h_T$. Since the input sequences’ lengths are unbounded, we have to put an *unbounded amount of information* into this vector. As the vector’s capacity is bounded, it becomes ***impossible*** for the decoder to produce sensible output sequences of arbitrary length. That's why we had so much difficulty producing good chinese translations with pure s2s.\n",
    "\n",
    "In other words, the architecture has a major **recency bias**, losing the signal from earlier on in the sentence in favor of “more relevant” recent words. This information, however, is not always more relevant. \n",
    "\n",
    "Take for example the sentence: *Inside the safari park, I walked over to pet a tiger*. Though the words *pet* and\n",
    "*walked over* are far more recent, it is clear that *safari park* is far more important to predicting that you actually went to pet a tiger.\n",
    "\n",
    "Also, the model is sequential and the output from one RNN layer is the input for the next RNN layer. In other words, we can’t run several computations at the same time since each computation depends on all previous ones. This makes training S-L-O-W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s2s with semi-Attention using (key, query, value) analogy\n",
    "**Attention** [Bahdanau, 2014] rids us of this requirement. Content-based attention mechanisms use the **entirety** of intermediate encoder states $h = (h_1, \\ldots, h_T)$ (the so-called **keys**) as a *memory* which can be queried using a so-called **query** vector (usually a decoder *word*).\n",
    "\n",
    "Attention lets the decoder focus on specific parts of the input sentence for each output word. This helps the input and output sentences to align with one another.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/s2s-decoding.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "**Attention weights** are derived in the following fashion:\n",
    "\n",
    "$$α_t =h_t^T q ∈ R$$\t\n",
    "\n",
    "for $t=1,\\ldots,T$. Intuitively, these can be interpreted as how *related* certain words are for some given query word $q$. \n",
    "\n",
    "Then, another component is defined: So-called **value** vectors $v = (v_1, \\ldots, v_T)$ can be conceived as being (possibly) non-linear transformations of $h = (h_1, \\ldots, h_T)$. \n",
    "\n",
    "For the sake of simplicity, let's assume that $v_t = h_t$  holds for all encoder timesteps.\n",
    "\n",
    "Putting all together we arrive at the **context vector**:\n",
    "\n",
    "$$w = ∑_{t=1}^T α_t′ v_t = ∑_{t=1}^T α_k′ h_t  ∈ R^{d_H}$$\n",
    "\n",
    "where $\\alpha_{t}' =\\frac{\\alpha_{t}}{\\sum_{k=1}^T \\alpha_{k}}$  is the **normalized attention weight**. In the best case, this context vector (also called a **summary**) $w$ contains salient information which aids us in generating the next symbol by supplying it as an input to the decoder.\n",
    "\n",
    "Breaking the dependence of relying on a fixed-length vector handoff between encoder and decoder has enabled researchers to improve their models by wide margins. And indeed, our s2s-with-attention translations were much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Attention\n",
    "Attention in practice is actually a very simple concept — it’s the idea how much you actually care about every piece of information, and weighing it accordingly in making a decision. The Transformer Neural Network does the same thing — it looks at every piece of information, decides how relevant it actually is to making a decision, and gives it a weight accordingly. Then, it weighs all information proportionally when actually making the decision. \n",
    "\n",
    "The type of Attention used in Transformers over just semi-Attention we already studied is unique because it computes Attention ***on itself*** (self-attention): Attention of the source sequence with itself, and then Attention of the target sequence with itself, on top of Attention of the target sequence with the source sequence. Essentially, it’s like a dictionary that constantly asks itself — *I wonder what parts of me would be most useful to whoever uses me to make a prediction*, and then proceeds to highlight those parts based on how important they are.\n",
    "\n",
    "**Scaled Dot Product Attention** is Self Attention that is computed as follows: each decoder word has a **Query vector** Q and a **Key vector** K. The dot product of these two vectors is called the **Value vector** V (take the dot product of Q and K and scale by an expression, hence the name **Scaled Dot Product**). The decoder uses a **masked** (past-context-reliant) version of this, since it can only look at words it has already decoded throughout the translation process.\n",
    "\n",
    ">**Note**: We are now *vectorizing* operations that used to be sequential in an RNN: Instead of looking at decoder words one by one, timestep by timestep, we look at all of them at the same time, applying an appropriate mask so that the future does not infect the past.\n",
    "\n",
    "**Multi-Headed Attention** is a repeated version of Scaled Dot Product Attention, applying Attention onto several copies of the same vector which have been filtered/ transformed in meaningful ways, then aggregating the results.\n",
    "\n",
    "Position doesn’t matter when actually weighting values as above, and all of them can be computed *simultaneously* over all words. But this is catastrophic when we actually build sentences, as words in a sentence do have a natural order! To solve this problem, we build **positional encodings** at the beginning of our model, so that position is kept track of and we can assemble the sentence in order.\n",
    "\n",
    "The order we must ensure is that our model takes these encodings into account (as input), yet doesn’t modify them and distort our order completely. For this, we build in **Residual**, or **skip** connections, so that we can both use the information as input for our next computational layer and simultaneously *skip it forward*. This allows our encodings to percolate through the network unmodified, so that all layers obtain sentence encodings that don’t sound like they had been Yoda by spoken (Yes, that was intentional).\n",
    "\n",
    "The last linear layer stretches our vector back into the dimensions of our input dictionary, each value/word assigned a certain weight with a softmax, assigning probabilities to every value in the vector. Ultimately, we pick the neuron with the highest probability, and that yields the output word.\n",
    "\n",
    "Once upon a time, someone built a tiny multi-million parameter model named a **Transformer**. The Transformer took in an input of words, **encoded** them, kept track of their position (positional encodings), and then learned which of the given words could be used to predict others using Self Attention. It did this by asking itself a survey of several questions, then aggregated the responses (Multi-Headed, Add + Norm). Once it had a meaningful vector encoding, it fed them into a **decoder**, which kept the unmodified original positional encodings by *skipping them over* (Residual Connections). It then proceeded to learn which parts of the encoder were most important for decoding, and repeated this whole gargantuan process 6 times. Finally, it came up with a giant vector. Since, sadly, no human could understand this vector, it widened it to the size of a dictionary (Linear Layer), calculated the probabilities of each word (Softmax Layer), and then predicted the most likely words at each position one by one. \n",
    "\n",
    "The Transformer is a revolutionary architecture in NLP. It has been used widely for top performing modern models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). Although it is quite\n",
    "complex, it is an elegant architecture that can be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A miniature and real-life language model pair\n",
    "\n",
    "We always begin with a miniature language model, so we can peek under the hood to see what transformer does! That is how one can debug ML models.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/double-model.jpg\" width=800 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our de-facto **中文** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tf 2.x, I do not need to enable eager execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "#import os\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "#import keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table('data/cmn.txt', names=['eng', 'cmn', 'org'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: x.lower())\n",
    "#lines.cmn = lines.cmn.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove end of lines periods: The regex is \\.$\n",
    "lines.eng = lines.eng.apply(lambda x: re.sub(\"\\.$\", '', x))\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(\"\\.$\", '', x))\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(\"。$\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: re.sub(r\"([!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~])\", r\" \\1 \", x))\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(r\"([!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~])\", r\" \\1 \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng = lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "lines.cmn = lines.cmn.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotes, saveguard commas:\n",
    "lines.eng = lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: re.sub('\"', '', x))\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub('\"', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: re.sub(\",\", ' COMMA', x)) #Note the extra space\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(\",\", 'COMMA', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: re.sub(r\"([!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~])\", r\" COMMA\", x)) #Note the extra space\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(r\"([!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~])\", r\"COMMA\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zhon\n",
    "from zhon import hanzi\n",
    "lines.cmn = lines.cmn.apply(lambda x: re.sub(r\"([\" + zhon.hanzi.punctuation + \"])\", r\" COMMA \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.cmn = lines.cmn.apply(lambda x: x[:-7] if x[-7:] == ' COMMA ' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.drop(['org'], axis=1, inplace=True)\n",
    "lines = lines.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zhconv\n",
    "from zhconv import convert\n",
    "lines.cmn = lines.cmn.apply(lambda x: convert(x , 'zh-cn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/gc/rb79dw2s4ll3nf9v72b36dtr0000gn/T/jieba.cache\n",
      "Loading model cost 0.788 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "lines.cmn = lines.cmn.apply(lambda x: \" \".join(jieba.cut(x, cut_all=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick 40 medium-length sentences from the middle of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>cmn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>underage drinking is a crime</td>\n",
       "      <td>未满 年龄 饮酒 是 罪行</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>unfortunately COMMA she is absent</td>\n",
       "      <td>不幸 的 是   COMMA   她 不 在</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>wait here till he comes back</td>\n",
       "      <td>在 这儿 等到 他 回来 为止</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>was there anyone in the room?</td>\n",
       "      <td>房间 里 有人 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>wash your hands before meals</td>\n",
       "      <td>饭前 要 洗手</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>we are badly in need of food</td>\n",
       "      <td>我们 非常 需要 食物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>we ate curry rice last night</td>\n",
       "      <td>我们 昨晚 吃 了 咖喱饭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>we can do more than they can</td>\n",
       "      <td>我们 能比 他们 做 得 多</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>we could see nothing but fog</td>\n",
       "      <td>除了 雾 我 看不见 任何 东西</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>we could walk there together</td>\n",
       "      <td>我们 可以 一起 走 着 去</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10022</th>\n",
       "      <td>we couldnt keep from crying</td>\n",
       "      <td>我们 止不住 哭泣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10023</th>\n",
       "      <td>we dont have another choice</td>\n",
       "      <td>我们 没有 别的 选择</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10024</th>\n",
       "      <td>we fixed that pretty quickly</td>\n",
       "      <td>我们 很快 修好 了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10025</th>\n",
       "      <td>we found the boy fast asleep</td>\n",
       "      <td>我们 发现 这个 男孩 睡得 很沉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10026</th>\n",
       "      <td>we got a good price for that</td>\n",
       "      <td>我们 得到 了 一个 很 好 的 价格 买 那个 东西</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>we got to the station at six</td>\n",
       "      <td>我们 六点钟 到 了 车站</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028</th>\n",
       "      <td>we had bad weather yesterday</td>\n",
       "      <td>昨天 天气 很糟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10029</th>\n",
       "      <td>we have a special connection</td>\n",
       "      <td>我们 有 特殊 联系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10030</th>\n",
       "      <td>we have to do something COMMA tom</td>\n",
       "      <td>汤姆   COMMA   我们 必须 要 做点 什么</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10031</th>\n",
       "      <td>we have to stop the bleeding</td>\n",
       "      <td>我们 必须 止血</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10032</th>\n",
       "      <td>we live in the united states</td>\n",
       "      <td>我们 住 在 美国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10033</th>\n",
       "      <td>we must conform to the rules</td>\n",
       "      <td>我们 必须 遵守规则</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10034</th>\n",
       "      <td>we must reduce energy demand</td>\n",
       "      <td>我们 必须 降低 能源需求</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10035</th>\n",
       "      <td>we set a trap to catch a fox</td>\n",
       "      <td>我们 设 了 个 陷阱 来 抓 狐狸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10036</th>\n",
       "      <td>we watched the children play</td>\n",
       "      <td>我们 看着 这 孩子 玩耍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>we went on a picnic together</td>\n",
       "      <td>我们 一起 去 野餐 了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>we werent making fun of you</td>\n",
       "      <td>我们 没有 笑话 你</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>we work every day but sunday</td>\n",
       "      <td>我们 除了 周日 每 一天 都 工作</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10040</th>\n",
       "      <td>well begin tomorrow morning</td>\n",
       "      <td>我们 明天 上午 开始</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10041</th>\n",
       "      <td>well go when the rain stops</td>\n",
       "      <td>当雨 停 了   COMMA   我们 就 会 去</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>well talk about it tomorrow</td>\n",
       "      <td>我们 明天 讨论 它</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>were both reasonable people</td>\n",
       "      <td>我们 是 两个 通情达理 的 人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10044</th>\n",
       "      <td>wearing a suit COMMA he stood out</td>\n",
       "      <td>他 穿着 西装 站 了 出来</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10045</th>\n",
       "      <td>well COMMA girls COMMA its time to go</td>\n",
       "      <td>那么   COMMA   女孩 们   COMMA   是 时候 出发 了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10046</th>\n",
       "      <td>what are my responsibilities?</td>\n",
       "      <td>我 的 责任 是 什么</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10047</th>\n",
       "      <td>what are you concerned about?</td>\n",
       "      <td>你 在 担心 什么 呢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10048</th>\n",
       "      <td>what did she buy at the shop?</td>\n",
       "      <td>她 在 店里 买 了 什么</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10049</th>\n",
       "      <td>what did you come here to do?</td>\n",
       "      <td>您 来 这儿 干嘛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10050</th>\n",
       "      <td>what did you go to kyoto for?</td>\n",
       "      <td>你 为什么 去 京都 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10051</th>\n",
       "      <td>what do you call this flower?</td>\n",
       "      <td>这个 花叫 什么 名字</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         eng  \\\n",
       "10012           underage drinking is a crime   \n",
       "10013      unfortunately COMMA she is absent   \n",
       "10014           wait here till he comes back   \n",
       "10015          was there anyone in the room?   \n",
       "10016           wash your hands before meals   \n",
       "10017           we are badly in need of food   \n",
       "10018           we ate curry rice last night   \n",
       "10019           we can do more than they can   \n",
       "10020           we could see nothing but fog   \n",
       "10021           we could walk there together   \n",
       "10022            we couldnt keep from crying   \n",
       "10023            we dont have another choice   \n",
       "10024           we fixed that pretty quickly   \n",
       "10025           we found the boy fast asleep   \n",
       "10026           we got a good price for that   \n",
       "10027           we got to the station at six   \n",
       "10028           we had bad weather yesterday   \n",
       "10029           we have a special connection   \n",
       "10030      we have to do something COMMA tom   \n",
       "10031           we have to stop the bleeding   \n",
       "10032           we live in the united states   \n",
       "10033           we must conform to the rules   \n",
       "10034           we must reduce energy demand   \n",
       "10035           we set a trap to catch a fox   \n",
       "10036           we watched the children play   \n",
       "10037           we went on a picnic together   \n",
       "10038            we werent making fun of you   \n",
       "10039           we work every day but sunday   \n",
       "10040            well begin tomorrow morning   \n",
       "10041            well go when the rain stops   \n",
       "10042            well talk about it tomorrow   \n",
       "10043            were both reasonable people   \n",
       "10044      wearing a suit COMMA he stood out   \n",
       "10045  well COMMA girls COMMA its time to go   \n",
       "10046          what are my responsibilities?   \n",
       "10047          what are you concerned about?   \n",
       "10048          what did she buy at the shop?   \n",
       "10049          what did you come here to do?   \n",
       "10050          what did you go to kyoto for?   \n",
       "10051          what do you call this flower?   \n",
       "\n",
       "                                         cmn  \n",
       "10012                          未满 年龄 饮酒 是 罪行  \n",
       "10013                 不幸 的 是   COMMA   她 不 在  \n",
       "10014                        在 这儿 等到 他 回来 为止  \n",
       "10015                              房间 里 有人 吗  \n",
       "10016                                饭前 要 洗手  \n",
       "10017                            我们 非常 需要 食物  \n",
       "10018                          我们 昨晚 吃 了 咖喱饭  \n",
       "10019                         我们 能比 他们 做 得 多  \n",
       "10020                       除了 雾 我 看不见 任何 东西  \n",
       "10021                         我们 可以 一起 走 着 去  \n",
       "10022                              我们 止不住 哭泣  \n",
       "10023                            我们 没有 别的 选择  \n",
       "10024                             我们 很快 修好 了  \n",
       "10025                      我们 发现 这个 男孩 睡得 很沉  \n",
       "10026            我们 得到 了 一个 很 好 的 价格 买 那个 东西  \n",
       "10027                          我们 六点钟 到 了 车站  \n",
       "10028                               昨天 天气 很糟  \n",
       "10029                             我们 有 特殊 联系  \n",
       "10030             汤姆   COMMA   我们 必须 要 做点 什么  \n",
       "10031                               我们 必须 止血  \n",
       "10032                              我们 住 在 美国  \n",
       "10033                             我们 必须 遵守规则  \n",
       "10034                          我们 必须 降低 能源需求  \n",
       "10035                     我们 设 了 个 陷阱 来 抓 狐狸  \n",
       "10036                          我们 看着 这 孩子 玩耍  \n",
       "10037                           我们 一起 去 野餐 了  \n",
       "10038                             我们 没有 笑话 你  \n",
       "10039                     我们 除了 周日 每 一天 都 工作  \n",
       "10040                            我们 明天 上午 开始  \n",
       "10041              当雨 停 了   COMMA   我们 就 会 去  \n",
       "10042                             我们 明天 讨论 它  \n",
       "10043                       我们 是 两个 通情达理 的 人  \n",
       "10044                         他 穿着 西装 站 了 出来  \n",
       "10045  那么   COMMA   女孩 们   COMMA   是 时候 出发 了  \n",
       "10046                            我 的 责任 是 什么  \n",
       "10047                            你 在 担心 什么 呢  \n",
       "10048                          她 在 店里 买 了 什么  \n",
       "10049                              您 来 这儿 干嘛  \n",
       "10050                           你 为什么 去 京都 ?  \n",
       "10051                            这个 花叫 什么 名字  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[10010:10050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_no_tags(w):\n",
    "    w = w.strip()\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(start, end):\n",
    "    sentence_pairs = [[preprocess_sentence_no_tags(l[0]), preprocess_sentence(l[1])] for l in lines[start:end].values]\n",
    "    return zip(*sentence_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Transformer, we add beginning and end of sentence sentinels to *both* languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_dataset():\n",
    "    #sentence_pairs = [[preprocess_sentence_no_tags(l[0]), preprocess_sentence(l[1])] for l in lines.values]\n",
    "    sentence_pairs = [[preprocess_sentence(l[0]), preprocess_sentence(l[1])] for l in lines.values]\n",
    "    return zip(*sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(start, end):\n",
    "    # creating cleaned input, output pairs\n",
    "    inp_lang, targ_lang = create_dataset(start, end)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_dataset():\n",
    "    # creating cleaned input, output pairs\n",
    "    inp_lang, targ_lang = create_full_dataset()\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if a person has not had a chance to acquire his target language by the time hes an adult COMMA hes unlikely to be able to reach native speaker level in that language <end>\n",
      "<start> 如果 一个 人 在 成人 前 没有 机会 习得 目标语言   COMMA   他 对 该 语言 的 认识 达到 母语 者 程度 的 机会 是 相当 小 的 <end>\n"
     ]
    }
   ],
   "source": [
    "en, zh = create_full_dataset()\n",
    "print(en[-1])\n",
    "print(zh[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples_p = 40 #len(lines)\n",
    "input_tensor_p, target_tensor_p, inp_lang_p, targ_lang_p = load_dataset(10010, 10050)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ_p, max_length_inp_p = target_tensor_p.shape[1], input_tensor_p.shape[1]\n",
    "max_length_targ_p, max_length_inp_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = len(lines)\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_full_dataset()\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36 4 4\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 90-10 split\n",
    "input_tensor_train_p, input_tensor_val_p, target_tensor_train_p, target_tensor_val_p = train_test_split(\n",
    "    input_tensor_p, target_tensor_p, test_size=0.1)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train_p), len(target_tensor_train_p), len(input_tensor_val_p), len(target_tensor_val_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19865 19865 2208 2208\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 90-10 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.1)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "47 ----> how\n",
      "126 ----> long\n",
      "38 ----> can\n",
      "20 ----> it\n",
      "33 ----> be\n",
      "497 ----> kept\n",
      "77 ----> going\n",
      "35 ----> with\n",
      "3111 ----> dollars?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "106 ----> 用\n",
      "446 ----> 美元\n",
      "47 ----> 可以\n",
      "1355 ----> 支持\n",
      "280 ----> 多久\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[30])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36, 8), (36, 13))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train_p.shape, target_tensor_train_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19865, 35), (19865, 29))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train.shape, target_tensor_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a large tf `dataset` as well as a miniature model using our own keras-style generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, min_index, max_index, shuffle=False, batch_size=128):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "    i = min_index\n",
    "    \n",
    "    while True:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "\n",
    "        # preallocate\n",
    "        samples = np.zeros((len(rows), data.shape[-1])) #1st dim:rows, 2nd dim:features\n",
    "        \n",
    "        # fill in: For each row selected, select the number of timesteps to sample from\n",
    "        for j, row in enumerate(rows):                         # for each observation (row)\n",
    "            indices = rows[j]\n",
    "            samples[j] = data[indices]\n",
    "            \n",
    "        yield samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with ***batches*** of 4 sequence pairs in our miniature model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_P = 4\n",
    "BUFFER_SIZE_P = len(input_tensor_train_p)\n",
    "\n",
    "inp_gen_p = generator(input_tensor_train_p,\n",
    "                    min_index=0,\n",
    "                    max_index=39,\n",
    "                    batch_size=BATCH_SIZE_P)\n",
    "\n",
    "trg_gen_p = generator(target_tensor_train_p,\n",
    "                    min_index=0,\n",
    "                    max_index=39,\n",
    "                    batch_size=BATCH_SIZE_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (4, 13))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch_p = next(inp_gen_p)\n",
    "example_target_batch_p = next(trg_gen_p)\n",
    "example_input_batch_p.shape, example_target_batch_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our representation of 4 source sequences in our miniature model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9., 116.,  29., 117.,   0.,   0.,   0.,   0.],\n",
       "       [  1.,  26.,   4.,  76.,  77.,  78.,  25.,   0.],\n",
       "       [ 35.,  17.,  36.,  18.,  37.,  38.,   0.,   0.],\n",
       "       [124., 125., 126., 127.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our representation of the respective 4 target sequences in our miniature model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   3.,  25., 107., 108.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.],\n",
       "       [  1.,   3.,  68.,   4.,  69.,  70.,  71.,   9.,  72.,  23.,  73.,\n",
       "         19.,   2.],\n",
       "       [  1.,   7.,  14.,  32.,  15.,  33.,  34.,   2.,   0.,   0.,   0.,\n",
       "          0.,   0.],\n",
       "       [  1.,   3.,   5., 115., 116.,   9., 117.,   2.,   0.,   0.,   0.,\n",
       "          0.,   0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_target_batch_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's peek at our ***full*** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 35), dtype=int32, numpy=\n",
       " array([[   1,   29,  136, ...,    0,    0,    0],\n",
       "        [   1,  153,   32, ...,    0,    0,    0],\n",
       "        [   1,   27,   18, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,   15, 2182, ...,    0,    0,    0],\n",
       "        [   1,    4,   41, ...,    0,    0,    0],\n",
       "        [   1,   15, 3261, ...,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(64, 29), dtype=int32, numpy=\n",
       " array([[   1, 1705,  538, ...,    0,    0,    0],\n",
       "        [   1,    3,  100, ...,    0,    0,    0],\n",
       "        [   1,   14,   38, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,   12, 4682, ...,    0,    0,    0],\n",
       "        [   1,    3,   25, ...,    0,    0,    0],\n",
       "        [   1,   12,  179, ...,    0,    0,    0]], dtype=int32)>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(dataset_val))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   1   10  257 ...    0    0    0]\n",
      " [   1  122   10 ...    0    0    0]\n",
      " [   1    9 1607 ...    0    0    0]\n",
      " ...\n",
      " [   1   15 1168 ...    0    0    0]\n",
      " [   1    3 1310 ...    0    0    0]\n",
      " [   1  701   13 ...    0    0    0]], shape=(64, 35), dtype=int32) tf.Tensor(\n",
      "[[   1    9 2591 ...    0    0    0]\n",
      " [   1   64    9 ...    0    0    0]\n",
      " [   1    7   32 ...    0    0    0]\n",
      " ...\n",
      " [   1   12 1129 ...    0    0    0]\n",
      " [   1 6655   10 ...    0    0    0]\n",
      " [   1    3    4 ...    0    0    0]], shape=(64, 29), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for (batch, (inp, tar)) in enumerate(dataset_train):\n",
    "    print(inp, tar)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introducing the Transformer model with pictures\n",
    "\n",
    "The **Transformer model** is a ***replacement*** of CNN/RNN architectures for sequence modeling, as introduced in the famous paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). \n",
    "\n",
    "It improved the state of the art for machine translation as well as natural language inference. Google's [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)#:~:text=Bidirectional%20Encoder%20Representations%20from%20Transformers,to%20better%20understand%20user%20searches.) was the first implementation of Transformer on large scale corpora, proving that it is capable of learning high-quality semantic representations. \n",
    "\n",
    "The interesting part of Transformer is its *extensive employment* of attention. \n",
    "\n",
    "The Transformer model additionally applies **self-attention** in both encoder and decoder, over the **attention** we already studied between the source and target sentences.\n",
    "\n",
    "This process forces words related to each other in some way to combine together irrespective of their positions in the sequence. \n",
    "\n",
    "This is in *addition* to our well-known word2vec-style **word embeddings**.\n",
    "\n",
    "However, in contrast to RNN-based models, where words in the source sentence are embedded and **hidden states** are passed on to the encoder, which is now undestood to be *overly constraining*, the classic Transformer model does not use RNNs nor hidden states.\n",
    "\n",
    "Also, the encoding component is actually a **stack of encoders** (the original transformer stacks *six* of them on top of each other). The decoding component is also a **stack of decoders** of the same number.\n",
    "\n",
    "The encoders are all identical in structure, they do not share weights, and each one is broken down into two sub-layers: **self-attention**, and a **dense layer**.\n",
    "\n",
    "The self-attention layer helps the encoder look at other words in the source input sentence as it encodes a specific word of that sentence, effectively *paying attention* at that sentence. It's like a miniature embedding layer, *just for that sentence*!.\n",
    "\n",
    "However, before self-attention, we laso have our well-known corpus-wide embedding layer. But it's only the *first* encoder that takes in the word embeddings, because the other encoders take in the output of the encoder that’s directly before.\n",
    "\n",
    "Decoders have the same two layers (self-attention and dense), but between the last encoder and the first decoder is an extra **attention layer** that helps the decoder focus on relevant parts of the input sentence. In other words, for us, each chinese word of the chinese sentence pays attention to each english word of its english translation. That attention layer is called **cross-attention**, or **encoder-decoder attention**.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer-encoder-decoder-2.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "Each word in the input sentence flows through its own path in the encoder. There are dependencies between words in the self-attention layer, but the dense feed-forward layer does not have those dependencies and thus the input words can be executed *in parallel* through the dense feed-forward layer. So now we're going to be able to leverage our GPU (if you have one)!\n",
    "\n",
    "## The mechanism of Attention\n",
    "\n",
    "RNN hidden states allow an RNN to incorporate the representation of previous words/vectors it has processed with the current one it’s processing. In contrast, self-attention is the method the Transformer uses to take into account past relevant words into the one currently processing.\n",
    "\n",
    "Say the following 2 sentences are input sentences we want to translate:\n",
    "\n",
    "`The animal didn't cross the street because it was too tired.`\n",
    "\n",
    "`The animal didn't cross the street because it was too wide.`\n",
    "\n",
    "What does `it` in these sentence refer to? Is it referring to `street` or to `animal`? When the model is processing the word `it`, self-attention allows it to associate `it` with `animal` or `street` (through training).\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/the-animal-and-the-street.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "The first sentence translates as:\n",
    "- `因为太累了, 该动物没有过马路` (Yīnwèi tài lèile, gāi dòngwù méiyǒuguò mǎlù)\n",
    "\n",
    "The second:\n",
    "- `该动物没有过马路因为它太宽了` (Gāi dòngwù méiyǒuguò mǎlù yīnwèi tā tài kuānle)\n",
    "\n",
    "The output of the decoder is *very different* depending on what the original english `it` is referring to. And self-attention gives the model the opportunity to learn the reference!\n",
    "\n",
    "Since we have many stacks of attention, as the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/animal-didnt-cross-the-street.png\" width=400 />\n",
    "</left>\n",
    "\n",
    "## Motivation: Latent Sematic Indexing as the origin of the linear algebra implementing Attention\n",
    "\n",
    "[Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_analysis) is a technique for creating a small number of **topics** to describe documents with. It is related to [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD), a dimensionality reduction technique in linear algebra, The picture below grasps the main idea:\n",
    "\n",
    "People can give each movie a rating of 1, 2, 3, 4, or 5. We can represent a people to movies matrix as a product of people to topics, topics to topics, and topics to movies. After we find the main topics and neglect the rest, we can compress each people to movie row to a topic representation, to compare how similar movies can be. This is how netflix movie recommendations work!\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/LSI.png\" width=600 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "The same process happens with word embeddings: As our words are embedded in a lower-dimensional space, they start belonging to ***topic groups*** that capture the semantics of the word. That is why word vectors `beautiful` and `pretty` are very close in that space, right next to chinese words `美丽` and `好看`. \n",
    "\n",
    "So, by multiplying input vectors with a matrix V (from the SVD), we obtain a better representation for computing the compatibility between these two vectors, in other words whether these two vectors are similar in topic space (as shown in the example in the figure), or not.\n",
    "\n",
    "This matrix can be computed with linear algebra, but it can also be learned in a neural network! And then all we have to do to obtain better latent representation of input vectors is matrix multiplication, which can be accomplished very efficiently in parallel!\n",
    "\n",
    "## Review: RNNs\n",
    "Here is a feed-forward (dense) neural network with one hidden layer. Since each layer is essentially a linear transformation and therefore a matrix multiplication. The 5D input are the 5 **features** of our observation. These get mapped to 4 hidden features, then transformed as 4 output features.\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/5x3x4.png\" width=600 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "An input of size 5 goes through a matrix multiplication with a 5x3 matrix (the weights), producing the input for the hidden layer, vector of size 3. Then, it gets multiplied by a 3x4 matrix, producing an output of 4.\n",
    "\n",
    "Let's add the area inside the dashed rectangle and one more 3x3 parameter matrix: We are adding one more input from the bottom. All inputs need to be of the same size. Because it also goes through the same blue matrix multiplication as the bottom one. Notice that the hidden output we had before gets multiplied by a square matrix before being combined with the new input. \n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/5x3x4 + loop.png\" width=900 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "Now, let’s extend the dashed area and make it a loop. Consider each blue input from the bottom as another observation at another timestep, so that instead of one observation, we have a *sequence* of observations $x_t$:\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/5x3x4 rnn.png\" width=900 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "What we are doing is applying a recurrent formula to every input from the sequence. What we have at the very end depends on the whole input sequence:\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/rnn-activation.png\" width=500 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/rnn-activation-2.png\" width=400 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "To picture it with the parameters we started with:\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/rnn-activation-5x3x4.png\" width=400 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "\n",
    "RNN are easy to learn, but hard to train. That is because the values in the hidden states tend to exponentially explode or vanish. More advanced RNNs such as LSTM and GRUs (the ones we used in our notebooks) mitigate that problem by adopting more sophisticated training mechanisms.\n",
    "\n",
    "## Implementing the Attention mechanism with Query, Key, and Value vectors\n",
    "\n",
    "Key/Value/Query triplets come from **retrieval systems**. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description etc.) associated with candidate videos in the database, then present you the best matched values (videos). It does not scan each picture frame in the video!\n",
    "\n",
    "In fact, we can reframe **sequence-2-sequence with attention** as a Key/Value/Query concept:\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/s2s-context-vector.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "### Cross-Attention\n",
    "The query here is from the decoder's hidden states, while key = value and comes from the encoder's hidden states. \n",
    "\n",
    "The score is the \"***compatibility***\" between the query and key, which in this case is the dot product between the query and key. The scores then go through a softmax function to yield a set of weights whose sum equals 1. Each weight multiplies its corresponding values to yield the **context vector** which utilizes all the encoder's hidden states.\n",
    "\n",
    "That in a nutshell is **seq2seq with attention**.\n",
    "\n",
    "### Self-Attention\n",
    "**Self-attention** works *differently*. The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a **query vector**, a **key vector**, and a **value vector**. \n",
    "\n",
    "These vectors are created by multiplying the embedding by three matrices that are trained during the training process. \n",
    "\n",
    "The figure below represents the computation of Output \\#3:\n",
    "<left>\n",
    "<img src=\"ipynb.images/sa-qkv.png\" width=800 />\n",
    "</left>\n",
    "\n",
    "Queries, keys, and values are transformations of the corresponding input state vectors, not related to any encoder or decoder hidden states. In fact, since neither our transfromer encoder nor decoder are RNNs, ***they have no hidden states***.\n",
    "\n",
    "Specifically, for word pair $(i,j)$ (from $i$ to $j$) with embedding $x_i, x_j ∈ R^n$, query, key, and value is defined as follows:\n",
    "\n",
    "$$\\begin{split}q_j = W_q\\cdot x_j \\\\\n",
    "k_i = W_k\\cdot x_i\\\\\n",
    "v_i = W_v\\cdot x_i\\end{split}$$\n",
    "\n",
    "where $W_q, W_k, W_v ∈ R^{n×d_k}$ map the embedded representation $x$ to `query`, `key`, and `value` space respectively, and are *trained* matrices.\n",
    "\n",
    "These new vectors are usually smaller in dimension than the embedding vector. For example, 64-dimensional when the embedding and encoder input/output vectors are 512-dimensional. \n",
    "\n",
    "Thus, an **attention function** can be defined as mapping a **query** and a set of **key-value pairs** to an **output**, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "Confused? Wait, it gets more complicated..\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/smiley-confused.jpg\" width=200 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "\n",
    "### Score for each word pair in the sequence\n",
    "The next step is to calculate a **score**, much like seq2seq attention. \n",
    "\n",
    "We need to score ***each*** word of the input sentence against ***each other*** word in the sentence. That score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "\n",
    "The score is calculated by taking the dot product of the query vector with the key vector of the word $i$ we’re scoring:\n",
    "\n",
    "$$\\textrm{score} = q_j^T k_i$$\n",
    "\n",
    "So if we’re processing self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2. The third score would be the dot product of q1 and k3, etc. So each word has as many scores as there are words in the sentence.\n",
    "\n",
    "The dot product essentially measures the **similarity** of a given query $q_j$ and a key $k_i$: If word $j$ needs the information stored in word $i$, the **query vector** at position $j$, ($q_j$), is supposed to be close to the **key vector** at position $i$, ($k_i$).\n",
    "\n",
    "There are other possibilities to implement the score function.\n",
    "\n",
    "### Weighted sum\n",
    "We then divide all scores by the square root of the dimension of the key vectors (e.g. 8 if the key vectors are 64-D). This leads to more stable gradients. We then pass the result through a softmax operation, normalized so that all scores are positive and add up to 1.\n",
    "\n",
    "This softmax classifying score determines how much each word will be expressed at this position. Clearly the word at position #1 will have the highest softmax score, but sometimes it’s useful to attend to another word in the sentence that is relevant to the current word.\n",
    "\n",
    "We then multiply each value vector by the softmax score, in order to amplify the word(s) we want to focus on and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
    "\n",
    "We then sum up the weighted value vectors. This produces the output of the self-attention layer for the first word.\n",
    "\n",
    "Coming from seq2seq eith attention, you may think about the weights $w_{ij}$ as our **attention weights** or **alignment vector**, about $Z$ as the **context vector**, and $o$ as our **attention vector** in the following computation:\n",
    "\n",
    "$$\\begin{split}w_{ji} = \\frac{\\exp\\{\\textrm{score}_{ji} \\}}{\\sum\\limits_{(k, i)\\in E}\\exp\\{\\textrm{score}_{ki} \\}} \\\\\n",
    "\\textrm{Z}_i = \\sum_{(k, i)\\in E} w_{ki} v_k \\\\\n",
    "o = W_o\\cdot \\textrm{Z} \\\\\\end{split}$$\n",
    "\n",
    "This is the computation in images:\n",
    "\n",
    "X is the embedding represeation of an input toekn (word).\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/self-attention-matrix-calculation.png\" width=400 />\n",
    "</left>\n",
    "\n",
    "This is followed by:\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/self-attention-softmax.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "$Z$, our self-attention context vector, is sometimes denoted as $wv$, and called **attention head**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-headed attention layer\n",
    "Now, about $Z$.. there is more than a single attention head $Z$ in the Transfomer model!\n",
    "\n",
    "In Transformer, attention is **multi-headed**. \n",
    "\n",
    "A **head** is very much like a **channel** in a CNN network. Multi-headed attention consists of multiple attention heads, in which each head refers to a single attention module $Z(i)$.\n",
    "\n",
    "Multi-headed attention gives the attention layer multiple **representation subspaces**. With multi-headed attention we have not just one, but *multiple* sets of Query/Key/Value weight matrices. The classic Transformer uses ***eight*** attention heads, so we end up with eight sets for each encoder/decoder. This expands the model’s ability to focus on different word pairs.\n",
    "\n",
    "Each of these attention heads is randomly initialized. Then, after training, each head is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n",
    "\n",
    "It's not very different from vision-based or speech-based CNNs, which do similar operations. In fact, I suspect the transformer model for NLP came from applying CNN ideas to NLP.\n",
    "\n",
    "Whereas `Thinking Machines` below would have been a sequence of two samples in our RNN model, it is now considered a holistic observation for our Transformer model, leading to 8 distinct Attention heads:\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_attention_heads.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "Since the feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word), we need a way to condense these eight down into a single matrix. So we concatenate the matrices (just like we concatenated the context vector with the GRU output in seq2seq with attention), then multiply them by an additional weights matrix $W_o$.\n",
    "\n",
    "All heads are concatenated and mapped to output $o$ with an affine (linear) layer:\n",
    "\n",
    "$$o = W_o \\cdot \\textrm{concat}\\left(\\left[\\textrm{Z}^{(0)}, \\textrm{Z}^{(1)}, \\cdots, \\textrm{Z}^{(h)}\\right]\\right)$$\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_attention_heads_weight_matrix_o2.png\" width=1200 />\n",
    "</left>\n",
    "\n",
    "How about we put everything into a single picture?\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_multi-headed_self-attention-recap.png\" width=1000 />\n",
    "</left>\n",
    "\n",
    "So here, for example, is the contribution to self-attention for ***two heads***:\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_self-attention_visualization_2.png\" width=500 />\n",
    "</left>\n",
    "\n",
    "As we encode the word `it`, one attention head is focusing mostly on `the animal`, while another is focusing on `tired`. In a sense, the model's representation of the word `it` connects to some of the representation of both `animal` and `tired`.\n",
    "\n",
    "And here's another representation of the overall process, with [brio](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html):\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/multiple-heads.gif\" width=700 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "But *wait*, we don't have RNNs anymore, so we also need a way to account for the *order* of words in the input sequence. Without this, we essentially have a Bag of Words (BOW) model.\n",
    "\n",
    "I did not process the sentence `Thinking Machines` above. I processed the sentence consisting of words `Thinking` and `Machine`, but its the same as the sentence `Machines Thinking`. So I need a process to specify word order in the sentence.\n",
    "\n",
    "To address this, the transformer adds a *special* vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_positional_encoding_vectors.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "Positional encodings are brilliant and scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set). They're described in the `Attention is all you need` paper.\n",
    "\n",
    "We'll go more into detail on this further down and in our Transformers review notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "Each sub-layer (self-attention, and dense feed-forward) of each encoder ***also*** has a **residual connection** around it, followed by a layer-normalization step. What is also known as a **resnet**.\n",
    "\n",
    "Residual connections are the same thing as **skip connections**. They are used to allow gradients to flow through a network directly, without passing through non-linear activation functions. It helps eliminate overtraining, much like we did with regularizers in our notebooks, because it essentially allows the network to dynamically learn the right number of layers for the learning task at hand.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_resideual_layer_norm_2.png\" width=500 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The decoder\n",
    "\n",
    "The encoders start by processing the input sequence. The output of the last encoder is then transformed into a set of attention vectors $K$ and $V$. These are to be used by each decoder in its **encoder-decoder attention** layer which helps the decoder focus on appropriate places in the input sequence:\n",
    "\n",
    "The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_decoding_1.gif\" width=700 />\n",
    "</left>\n",
    "\n",
    "The self attention layers in the decoder operate in a slightly different way than the one in the encoder: In the decoder, *the self-attention layer is only allowed to attend to earlier positions in the output sequence*. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "\n",
    "The **Encoder-Decoder Attention** layer works just like multiheaded self-attention, except it creates its Queries matrix from the decoder layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n",
    "\n",
    "And just as with seq2seq with attention, the decoder goes one-word at a time through the labelled target sentence in order to predcit the next word and train all the decodeer's weights in the process! The steps below repeat until the end sentinel symbol `<end>` is reached, indicating the transformer decoder has completed its output: \n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/transformer_decoding_2.gif\" width=700 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Softmax Layer\n",
    "The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer, followed by a Softmax. We used the same final layer in our encoder-decoder-wth-attention model.\n",
    "\n",
    "This Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much larger vector called a **logits vector**.\n",
    "\n",
    "If our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset, this would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n",
    "\n",
    "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen (through a `np.argmax()` operation), and the word associated with it is produced as the output for this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "\n",
    "So, if Im outputting `迪诺 喜欢 走走 公园` (spaces added by me) from `dino likes to walk in the park`, then as we translate `dino`, we want the first probability distribution to have the highest probability at the cell associated with the word `迪诺`, the second probability distribution to have the highest probability at the cell associated with the word `喜欢`, and so on, until the fifth output distribution indicates the `<end>` sentinel, which also has a cell associated with it from our target vocabulary.\n",
    "\n",
    "Now, because the model produces the output words one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it, called **greedy decoding**. \n",
    "\n",
    "Another way to do it would be something similiar to CTC in a speech notebook: Hold on to, say, the top two words, then in the next step, run the model *twice*: Once assuming the first output position was the word `迪诺`, and another time assuming the first output position was the word ·`冬`, and whichever version produced less error considering both positions #1 and #2 are kept. \n",
    "\n",
    "We repeat this for positions #2 and #3…etc. This method is called [beam search](https://en.wikipedia.org/wiki/Beam_search), where in our example, beam_size was two, meaning that two partial hypotheses (unfinished translations) are kept in memory, and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters to be experimented with.\n",
    "\n",
    "## OMG\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/smiley-worn-out.jpg\" width=100 />\n",
    "</left>\n",
    "\n",
    "Wasn't that super complicated?\n",
    "\n",
    "There are many excellent descriptions of Transformer on the Web. One that I like and one of the most famous ones is Jay Alammar's [illustrated Transformer](http://jalammar.github.io/illustrated-transformer/). All the pictures above in this notebook come from Jay! Please read his post.\n",
    "\n",
    "And here's an excellent video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/rBCqOTEfxvg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x22c436135c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('rBCqOTEfxvg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A graph neural network\n",
    "\n",
    "To express the Transformer model, some publications like to treat attention as **edges** (connections between nodes) in a graph, and adopt message passing on the edges to induce the appropriate processing.\n",
    "\n",
    "When each word $s_i$ can attend to any other word $s_j$ (including self-loops), you get the following graph:\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/source-self-attention.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "The target language graph is **half-complete**, in that $t_i$ attends to $t_j$ if $i>j$ (an output word cannot depend on future words because the the decoder loop is unidirectional).\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/target-self-attention.png\" width=600 />\n",
    "</left>\n",
    "\n",
    ">**Note**: I thought a bidirectional decoder would be a good research topic, because I do expect it may provide an additional advantage. In german for example, the verb is always at the end of the sentence, and depending on the verb, I may want to decline the subject (which precedes it) differently. And then I discovered that BERT *is* bidirectional!\n",
    "\n",
    "The cross-language graph is a **bi-partitie graph**, where there is an edge from every source word $s_i$ to every target word $t_j$, meaning every target word can attend on source words.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/cross-attention.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "So the processing flow of Transformer is a 2-stage **message-passing** architecture within the complete graph:\n",
    "\n",
    "- 1) Self-attention in encoder\n",
    "- 2) Self-attention in decoder, followed by cross-attention between encoder and decoder\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/full-transformer.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "\n",
    "### Preprocessing \n",
    "The Transfomer's preprocessing function `pre_func` first normalizes words representations, and then maps them to the set of queries, keys and values we talked about:\n",
    "\n",
    "$$\\begin{split}x \\leftarrow \\textrm{LayerNorm}(x) \\\\\n",
    "[q, k, v] \\leftarrow [W_q, W_k, W_v ]\\cdot x\\end{split}$$\n",
    "\n",
    "### Postprocessing \n",
    "The postprocessing function `post_func` completes the whole computation corresponding to one layer of the transformer: \n",
    "\n",
    "- 1) Normalize $Z$ and get the output of Multi-Head Attention Layer $o$\n",
    "\n",
    "$$\\begin{split}\\textrm{Z} \\leftarrow \\frac{\\textrm{Z}}{z} \\\\\n",
    "o \\leftarrow W_o\\cdot \\textrm{Z} + b_o\\end{split}$$\n",
    "\n",
    "- 2) Apply a two layer position-wise feed forward layer on $x$, then add residual connection:\n",
    "\n",
    "$$x \\leftarrow x + \\textrm{LayerNorm}(\\textrm{FFN}(x))$$\n",
    "\n",
    "where FFN refers to the feed forward function.\n",
    "\n",
    "### A bit of code\n",
    "The code below is from the `torch` attention library. Just to give you an idea. No need to study this *in depth*, we will continue to use  `tensorflow` for now.\n",
    "\n",
    "```(python)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def pre_func(self, i, fields='qkv'):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            ...\n",
    "        return func\n",
    "\n",
    "    def post_func(self, i):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            ...\n",
    "        return func\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def pre_func(self, i, fields='qkv', l=0):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            ...\n",
    "        return func\n",
    "\n",
    "    def post_func(self, i, l=0):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            ...\n",
    "        return func\n",
    "```\n",
    "\n",
    "For each node $i$ (no matter whether it is a source token or target word), you can decompose the attention computation into two steps:\n",
    "\n",
    "- **Message computation**: Compute attention score `score`$_{ij}$ between $i$ and all nodes $j$ to be attended over, by taking the scaled-dot product between $q_i$ and $k_j$. The message sent from $j$ to $i$ will consist of the score `score`$_{ij}$ and the value $v_j$.\n",
    "- **Message aggregation**: Aggregate the values $v_j$ from all $j$ according to scores `score`$_{ij}$.\n",
    "\n",
    "This is how data is pre- and post-processed before and after the `propagate_attention()` functionove. Note: $wv$ = our $Z$ above.\n",
    "\n",
    "```(python)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_enc, generator, h, d_k):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder, self.decoder = encoder, decoder\n",
    "        self.src_embed, self.tgt_embed = src_embed, tgt_embed\n",
    "        self.pos_enc = pos_enc\n",
    "        self.generator = generator\n",
    "        self.h, self.d_k = h, d_k\n",
    "\n",
    "    def propagate_attention(self, g, eids):\n",
    "        # Compute attention score\n",
    "        g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n",
    "        g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n",
    "        # Send weighted values to target nodes\n",
    "        g.send_and_recv(eids,\n",
    "                        [fn.src_mul_edge('v', 'score', 'v'), fn.copy_edge('score', 'score')],\n",
    "                        [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n",
    "        \n",
    "    def update_graph(self, g, eids, pre_pairs, post_pairs):\n",
    "        \"Update the node states and edge states of the graph.\"\n",
    "        # Pre-compute queries and key-value pairs.\n",
    "        for pre_func, nids in pre_pairs:\n",
    "            g.apply_nodes(pre_func, nids)\n",
    "        self.propagate_attention(g, eids)\n",
    "        # Further calculation after attention mechanism\n",
    "        for post_func, nids in post_pairs:\n",
    "            g.apply_nodes(post_func, nids)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        g = graph.g\n",
    "        nids, eids = graph.nids, graph.eids\n",
    "\n",
    "        # Word Embedding and Position Embedding\n",
    "        src_embed, src_pos = self.src_embed(graph.src[0]), self.pos_enc(graph.src[1])\n",
    "        tgt_embed, tgt_pos = self.tgt_embed(graph.tgt[0]), self.pos_enc(graph.tgt[1])\n",
    "        g.nodes[nids['enc']].data['x'] = self.pos_enc.dropout(src_embed + src_pos)\n",
    "        g.nodes[nids['dec']].data['x'] = self.pos_enc.dropout(tgt_embed + tgt_pos)\n",
    "\n",
    "        for i in range(self.encoder.N):\n",
    "            # Step 1: Encoder Self-attention\n",
    "            pre_func = self.encoder.pre_func(i, 'qkv')\n",
    "            post_func = self.encoder.post_func(i)\n",
    "            nodes, edges = nids['enc'], eids['ee']\n",
    "            self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n",
    "\n",
    "        for i in range(self.decoder.N):\n",
    "            # Step 2: Dncoder Self-attention\n",
    "            pre_func = self.decoder.pre_func(i, 'qkv')\n",
    "            post_func = self.decoder.post_func(i)\n",
    "            nodes, edges = nids['dec'], eids['dd']\n",
    "            self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n",
    "            \n",
    "            # Step 3: Encoder-Decoder attention\n",
    "            pre_q = self.decoder.pre_func(i, 'q', 1)\n",
    "            pre_kv = self.decoder.pre_func(i, 'kv', 1)\n",
    "            post_func = self.decoder.post_func(i, 1)\n",
    "            nodes_e, nodes_d, edges = nids['enc'], nids['dec'], eids['ed']\n",
    "            self.update_graph(g, edges, [(pre_q, nodes_d), (pre_kv, nodes_e)], [(post_func, nodes_d)])\n",
    "\n",
    "        return self.generator(g.ndata['x'][nids['dec']])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention matrix examples\n",
    "\n",
    "In self-encoding attention, different heads learn different relations between word pairs, in the same way CNNs learn multi-resolution structure in an image. \n",
    "\n",
    "Here's an english-german example of self-encoder attention matrices:\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/encoder-self-attention-c.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "In encoder-decoder cross-attention, most words in the target sequence attend on their related words in the source sequence:\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/encoder-self-attention-c.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "In decoder self-attention. most words attend on their previous few words:\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/decoder-self-attention-c.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "If you think this is *complicated*, Google recently introduced the [Universal Transformer](https://arxiv.org/pdf/1807.03819.pdf) to address the problem that vanilla Transformer is *not computationally universal enough*. It essentially introduced recurrence back into Transformer, after having taken it out from seq2seq with attention!\n",
    "\n",
    "The basic idea of Universal Transformer is to repeatedly revise its representations of *all* words in the sequence with each recurrent step by applying a Transformer layer on the representations.\n",
    "\n",
    "Compared to vanilla Transformer, Universal Transformer *shares weights among its layers*, and does not fix recurrence time (number of layers in Transformer).\n",
    "\n",
    "[Here's](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer) the reference for all this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tensorflow Transformer\n",
    "\n",
    "The tensorflow [transformer](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb) is more complicated than our seq2seq-with-Attention model!\n",
    "\n",
    "But I think we may be ready to start coding it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUFFER_SIZE = 20000\n",
    "#BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_train\n",
    "val_dataset = dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 35), dtype=int32, numpy=\n",
       " array([[   1,   37,  460, ...,    0,    0,    0],\n",
       "        [   1,    4,  541, ...,    0,    0,    0],\n",
       "        [   1,   20,   21, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,   30,   54, ...,    0,    0,    0],\n",
       "        [   1,   55, 1793, ...,    0,    0,    0],\n",
       "        [   1,    9,   59, ...,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(64, 29), dtype=int32, numpy=\n",
       " array([[ 1, 12,  4, ...,  0,  0,  0],\n",
       "        [ 1,  3, 15, ...,  0,  0,  0],\n",
       "        [ 1, 53, 42, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 18, 51, ...,  0,  0,  0],\n",
       "        [ 1, 40, 32, ...,  0,  0,  0],\n",
       "        [ 1,  7,  8, ...,  0,  0,  0]], dtype=int32)>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_batch, en_batch = next(iter(val_dataset))\n",
    "zh_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we describe the encoder and decoder, let's work out a few details.\n",
    "\n",
    "## Now for the genius of positional encoding\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/toon-einstein.png\" width=200 />\n",
    "</left>\n",
    "\n",
    "**Order of words** are essential parts of any language. They define **grammar** and thus the actual **semantics** of a sentence. \n",
    "\n",
    "Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But embeddings do not encode the relative position of words in a sentence. However, that is not the responsibility of the embedding layer. It is the responsibility of the **recurrence** or **convolution** layer.\n",
    "\n",
    "But the Transformer model doesn't contain any recurrence or convolution, so **positional encoding** needs to be added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    ">**An example**: If the sequence is `I think therefore I am`, the output representation for the second `I` is not the same as the output representation for the first `I` because the hidden state that is inputted into these words are not the same: For the second `I`, the hidden state has passed though the words `I think therefore` while for the first `I` the hidden state is just initialized. RNN’s hidden state ensures that identical words that are in different positions in an input sequence will have distinct output representations. But the self-attention layer of a Transformer (without any positional representation) causes identical words at different positions to have the same output representation. If the input sequence `I think therefore I am` is passed through just one Transformer, although the two `I`s are located at different positions in the input sequence, the inputs for their respective output representations are identical because each `I` is compared against every other word in the sentence.\n",
    "\n",
    "So, a positional encoding:\n",
    "- Should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "- The distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "- The model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "- Must be deterministic.\n",
    "\n",
    "One idea is to assign a number to each time-step linearly. That is, the first word is given `1`, the second word is given `2`, and so on. The problem with this approach is that not only the values could get quite large, but also our model can face sentences longer than the ones in training.\n",
    "\n",
    "Let's look at the binary numbers we used in our additon notebooks:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  2: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\ \n",
    "  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Can you see the rate of change between different bits? The LSB bit alternates on ***every*** number, the second-lowest bit alternates every ***two*** numbers, and so on.\n",
    "\n",
    "But using binary values would be a waste of space in the world of floats. So instead, let's use their float continous counterparts: **Sinusoidal functions**. Indeed, they are the equivalent to alternating bits. Moreover, By decreasing their frequencies, we can go from red bits to orange ones!\n",
    "\n",
    "The formula for calculating positional encoding is:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$\n",
    "\n",
    "That positional encoding vector is just ***added*** to the embedding vector. \n",
    "\n",
    "Why added?\n",
    "\n",
    "Only the first few dimensions of the whole embedding are used to store the information about positions (see example plot below). And since the embeddings in the Transfomer are trained from scratch,  parameters are probably set in a way that the semantics of words do not get stored in the first few dimensions to avoid interfering with positional encoding.\n",
    "\n",
    "After adding positional encoding, in d-dimensional space, words will be closer to each other based on the *similarity of their meaning*, but also based on their *position in the sentence*.\n",
    "\n",
    "But wait! Won't position information vanish out once it reaches the upper layers in the encoder and decoder? Glad you asked! We will see that the Transformer architecture is equipped with **residual connections**. Therefore, information from the input of the model (which contains the positional embeddings) can efficiently propagate to further layers where more complex interactions are handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of positional encoding  for a sentence with the maximum length of 50 in 512-dimensional positonal encoding. Each row represents the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-dab2d1815284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpos_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpcolormesh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RdBu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Depth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d23606bba4d7>\u001b[0m in \u001b[0;36mpositional_encoding\u001b[1;34m(position, d_model)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mpos_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mangle_rads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column vectors $e(t)$ encode the position $t$ in an input embedding sequence of length $n$, where $d_\\text{model}$ is the dimension of the embedding: \n",
    "\n",
    "$$\\begin{equation}e(t) = \\boldsymbol{E}_{t,:} := \\begin{bmatrix} \n",
    "\\sin\\left(\\frac{t}{f_1}\\right)\\\\ \n",
    "\\cos\\left(\\frac{t}{f_1}\\right)\\\\ \n",
    "\\sin\\left(\\frac{t}{f_2}\\right)\\\\ \n",
    "\\cos\\left(\\frac{t}{f_2}\\right)\\\\ \n",
    "\\vdots\\\\ \n",
    "\\sin\\left(\\frac{t}{f_{\\frac{d_\\text{model}}{2}}}\\right)\\\\ \n",
    "\\cos\\left(\\frac{t}{f_{\\frac{d_\\text{model}}{2}}}\\right) \n",
    "\\end{bmatrix}\\,,\\tag{1}\\end{equation}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$f_m = \\frac{1}{\\lambda_m} := 10000^{\\frac{2m}{d_\\text{model}}}\\,.\\tag{2}$$\n",
    "\n",
    "A matrix $\\boldsymbol{T}^{(k)}$ exists such that for any $k$ and $t$:\n",
    "\n",
    "$$\\boldsymbol{T}^{(k)}\\boldsymbol{E}_{t,:}=\\boldsymbol{E}_{t+k,:}\\tag{3}$$\n",
    "\n",
    "If you take that matrix to be:\n",
    "\n",
    "$$\\begin{equation}\\boldsymbol{T}^{(k)} = \\begin{bmatrix} \n",
    "\\boldsymbol{\\Phi}^{(k)}_1 & \\boldsymbol{0} & \\cdots & \\boldsymbol{0} \\\\ \n",
    "\\boldsymbol{0} & \\boldsymbol{\\Phi}^{(k)}_2 & \\cdots & \\boldsymbol{0} \\\\ \n",
    "\\boldsymbol{0} & \\boldsymbol{0} & \\ddots & \\boldsymbol{0} \\\\ \n",
    "\\boldsymbol{0} & \\boldsymbol{0} & \\cdots & \\boldsymbol{\\Phi}^{(k)}_{\\frac{d_\\text{model}}{2}} \n",
    "\\end{bmatrix}\\,,\\tag{4}\\end{equation}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\begin{equation}\\boldsymbol{\\Phi}^{(k)}_m = \\begin{bmatrix} \n",
    "\\cos(r_mk) & -\\sin(r_mk) \\\\ \n",
    "\\sin(r_mk) & \\cos(r_mk) \n",
    "\\end{bmatrix}^\\intercal\\,,\\tag{5}\\end{equation}$$\n",
    "\n",
    "Then because:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\sin \\left( {\\alpha + \\beta } \\right) &= \\sin \\alpha \\cos \\beta + \\cos \\alpha \\sin \\beta\\tag{7a}\\\\ \n",
    "\\cos \\left( {\\alpha + \\beta } \\right) &= \\cos \\alpha \\cos \\beta – \\sin \\alpha \\sin \\beta\\tag{7b} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$r_m = \\lambda_m$ and $\\boldsymbol{T}^{(k)}$ is fully specified and depends solely on $m$, $d_\\text{model}$, and $k$. The position within the sequence, $t$, is not a parameter. And so for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. And so the model can easily learn to attend to by relative positions.\n",
    "\n",
    "Here is a quote from the original paper:\n",
    "\n",
    ">**[excerpt]**: We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking pad tokens\n",
    "\n",
    "We also need to mask all the pad tokens in our sequences. Do you remember the problem we had when we worked with a batch of sequences and had to keep processing `0` tokens (past the `<end>` sentinel) in the decoder loop (in fact, I had a bug in my code related to that)? \n",
    "\n",
    "We need to do a better job ensuring that our model does not treat padding as an input. So we use a mask to indicate where pad value `0` is present: Output a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a padding mask. Notice how there are `1`s where there are `0`s in the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look-ahead mask is also used to mask future tokens in a sequence. In other words, that mask indicates which entries should not be used.\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/look-ahead-mask.png\" width=300 />\n",
    "</left>\n",
    "\n",
    "This means that to predict the third word, only the first and second word can be used. To predict the fourth word, only the first, second and the third word will be used, and so on.\n",
    "\n",
    "Using the look-ahead mask yields more useful scores for the decoder:\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/masked-scores.png\" width=700 />\n",
    "</left>\n",
    "\n",
    "Once you take the softmax of the masked scores, the negative infinities get zeroed out, leaving zero attention scores for future tokens:\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/masked-scores-example.png\" width=700 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights and which we described a bit above is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of the square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude, straining the softmax function where it has small gradients and resulting in a very harsh softmax. \n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/scaled_attention.png\" width=400 />\n",
    "</left>\n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and thus you get a gentler softmax (not driving to `0` or `1` as much).\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an examples of queries given a key and value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self-attention mechanism directly models relationships between all words in a sentence, regardless of their respective position. In the sentence `I arrived at the bank after crossing the river`, to determine that the word `bank` refers to the shore of a river and not a financial institution, the Transformer can learn to immediately attend to the word `river` and make this decision in a single step. \n",
    "\n",
    "More specifically, to compute the next representation for a given word - `bank` for example - the Transformer compares it to ***every other word*** in the sentence. The result of these comparisons is an attention score for every other word in the sentence. These attention scores determine how much each of the other words should contribute to the next representation of `bank`. In the example, the disambiguating `river` could receive a high attention score when computing a new representation for `bank`. The attention scores are then used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation for `bank`, reflecting that the sentence is talking about a river bank.\n",
    "\n",
    "This is the process:\n",
    "- Embeddings go through dense layers to create q,k,v.\n",
    "- Query and key are used to compute scores\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/qkv.gif\" width=450 />\n",
    "</left>\n",
    "<br/>\n",
    "\n",
    "- This is what scores may look like for the sentence `hi how are you`:\n",
    "<left>\n",
    "<img src=\"ipynb.images/self-attention-example.png\" width=400 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "- Following a softmax activation:\n",
    "<left>\n",
    "<img src=\"ipynb.images/softmax-self-attention-example.png\" width=400 />\n",
    "</left>\n",
    "<br />\n",
    "\n",
    "- The output of the self-attention layer is then fed though the next encoder or decoder:\n",
    "<left>\n",
    "<img src=\"ipynb.images/self-attention-output.png\" width=700 />\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "\n",
    "Instead of a single attention head, Q, K, and V are split into multiple heads because that allows the model to jointly attend to information at different positions from different representational spaces. \n",
    "\n",
    "So Multi-head attention consists of four parts:\n",
    "*    Linear layers split into 8 heads\n",
    "*    8 Scaled dot-product attentions\n",
    "*    Concatenation of heads\n",
    "*    Final linear layer\n",
    "\n",
    "<left>\n",
    "<img src=\"ipynb.images/multi_head_attention.png\" width=400 />\n",
    "</left>\n",
    "\n",
    "After the split, each head has a reduced dimensionality so the total computation cost is the same as a single head attention with full dimensionality.\n",
    "\n",
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (`broadcast`ed for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "    \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise feed forward network\n",
    "\n",
    "A pointwise feed forward network consists of two fully-connected layers with a ReLU activation in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and decoder\n",
    "\n",
    "Now we're ready to talk about the encoder and decoder. In that, the transformer model follows the same general pattern as our standard sequence to sequence with attention model, albeit with multiple layers:\n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generate an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. \n",
    "\n",
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1. Multi-head attention (with padding mask) \n",
    "2. Pointwise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1. Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2. Multi-head attention (with padding mask): V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3. Pointwise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "And now we can talk about the encoder.\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Word2vec semantic embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is fed through an embedding which is then summed with  positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The decoder\n",
    "\n",
    " The `Decoder` consists of:\n",
    "1.   Word2vec embedding of the target sequence\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target sequence is fed through an embedding and summed with positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to our final linear layer, which we mention in the wrapper Transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Transformer\n",
    "\n",
    "Transformer consists of the encoder, decoder and our final affine (linear) `Dense` layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "To keep this notebook small and relatively fast,  values for `num_layers`, `d_model`, and `dff` have been reduced. \n",
    "\n",
    "The values used in the base model of transformer are: `num_layers = 6`, `d_model = 512`, and `dff = 2048`. See [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    ">**Note**: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "#input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "#target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "input_vocab_size = len(inp_lang.word_index) + 1\n",
    "target_vocab_size = len(targ_lang.word_index) + 1\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19865, 7485)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tensor_train), input_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19865, 10717)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tensor_train), target_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n",
    "\n",
    "We also used a learning in our CTC speech notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Train Step')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x149f12a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics\n",
    "\n",
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs.\n",
    "\n",
    "Create the `transformer` folder in your `~/data/training_checkpoints` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"data/training_checkpoints/transformer\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is divided into `tar_inp` and `tar_real`. `tar_inp` is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"<start> A lion in the jungle is sleeping <end>\"\n",
    "\n",
    "`tar_inp` =  `<start> A lion in the jungle is sleeping`\n",
    "\n",
    "`tar_real` = `A lion in the jungle is sleeping <end>`\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one word at a time, and uses its output so far to decide what word to predict next. \n",
    "\n",
    "During training, this example uses teacher-forcing \n",
    "\n",
    "(as in the [text generation tutorial](./text_generation.ipynb)). \n",
    "\n",
    "Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, **self-attention** allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output, the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "# for bigger corpuses\n",
    "#train_step_signature = [\n",
    "#    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#]\n",
    "\n",
    "# for our corpus\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our training loop:\n",
    "\n",
    "### Before you run this, run the cells below, which define the translation routine\n",
    "We'll be running translations inline as we're training the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0940 Accuracy 0.2383\n",
      "Epoch 1 Batch 50 Loss 0.1172 Accuracy 0.2333\n",
      "Epoch 1 Batch 100 Loss 0.1287 Accuracy 0.2326\n",
      "Epoch 1 Batch 150 Loss 0.1405 Accuracy 0.2315\n",
      "Epoch 1 Batch 200 Loss 0.1551 Accuracy 0.2311\n",
      "Epoch 1 Batch 250 Loss 0.1722 Accuracy 0.2307\n",
      "Epoch 1 Batch 300 Loss 0.1921 Accuracy 0.2300\n",
      "Epoch 1 Loss 0.1962 Accuracy 0.2299\n",
      "Time taken for 1 epoch: 160.19409584999084 secs\n",
      "\n",
      "<start> there was a big fire last night <end>\n",
      "[[1, 49, 21, 7, 256, 354, 92, 164, 2]]\n",
      "Epoch 2 Batch 0 Loss 0.1102 Accuracy 0.2277\n",
      "Epoch 2 Batch 50 Loss 0.1272 Accuracy 0.2348\n",
      "Epoch 2 Batch 100 Loss 0.1330 Accuracy 0.2331\n",
      "Epoch 2 Batch 150 Loss 0.1433 Accuracy 0.2323\n",
      "Epoch 2 Batch 200 Loss 0.1570 Accuracy 0.2313\n",
      "Epoch 2 Batch 250 Loss 0.1722 Accuracy 0.2310\n",
      "Epoch 2 Batch 300 Loss 0.1906 Accuracy 0.2303\n",
      "Epoch 2 Loss 0.1946 Accuracy 0.2301\n",
      "Time taken for 1 epoch: 161.558207988739 secs\n",
      "\n",
      "<start> i dont care a bit about the future <end>\n",
      "[[1, 4, 24, 320, 7, 516, 63, 3, 1040, 2]]\n",
      "Epoch 3 Batch 0 Loss 0.1554 Accuracy 0.2383\n",
      "Epoch 3 Batch 50 Loss 0.1257 Accuracy 0.2340\n",
      "Epoch 3 Batch 100 Loss 0.1343 Accuracy 0.2319\n",
      "Epoch 3 Batch 150 Loss 0.1443 Accuracy 0.2318\n",
      "Epoch 3 Batch 200 Loss 0.1545 Accuracy 0.2314\n",
      "Epoch 3 Batch 250 Loss 0.1701 Accuracy 0.2308\n",
      "Epoch 3 Batch 300 Loss 0.1878 Accuracy 0.2303\n",
      "Epoch 3 Loss 0.1906 Accuracy 0.2302\n",
      "Time taken for 1 epoch: 161.64505100250244 secs\n",
      "\n",
      "<start> i didnt have to study yesterday <end>\n",
      "[[1, 4, 70, 18, 5, 264, 138, 2]]\n",
      "Epoch 4 Batch 0 Loss 0.0949 Accuracy 0.2282\n",
      "Epoch 4 Batch 50 Loss 0.1215 Accuracy 0.2339\n",
      "Epoch 4 Batch 100 Loss 0.1269 Accuracy 0.2325\n",
      "Epoch 4 Batch 150 Loss 0.1392 Accuracy 0.2317\n",
      "Epoch 4 Batch 200 Loss 0.1521 Accuracy 0.2317\n",
      "Epoch 4 Batch 250 Loss 0.1694 Accuracy 0.2311\n",
      "Epoch 4 Batch 300 Loss 0.1889 Accuracy 0.2303\n",
      "Epoch 4 Loss 0.1922 Accuracy 0.2301\n",
      "Time taken for 1 epoch: 161.4791841506958 secs\n",
      "\n",
      "<start> my camera doesnt need to be fixed <end>\n",
      "[[1, 13, 472, 95, 105, 5, 33, 1619, 2]]\n",
      "Epoch 5 Batch 0 Loss 0.1306 Accuracy 0.2104\n",
      "Epoch 5 Batch 50 Loss 0.1255 Accuracy 0.2320\n",
      "Epoch 5 Batch 100 Loss 0.1329 Accuracy 0.2314\n",
      "Epoch 5 Batch 150 Loss 0.1438 Accuracy 0.2321\n",
      "Epoch 5 Batch 200 Loss 0.1548 Accuracy 0.2317\n",
      "Epoch 5 Batch 250 Loss 0.1718 Accuracy 0.2311\n",
      "Epoch 5 Batch 300 Loss 0.1903 Accuracy 0.2303\n",
      "Saving checkpoint for epoch 5 at data/training_checkpoints/transformer/ckpt-17\n",
      "Epoch 5 Loss 0.1922 Accuracy 0.2303\n",
      "Time taken for 1 epoch: 162.09836101531982 secs\n",
      "\n",
      "<start> which is larger COMMA japan or england? <end>\n",
      "[[1, 247, 8, 1520, 14, 260, 178, 5862, 2]]\n",
      "Epoch 6 Batch 0 Loss 0.1315 Accuracy 0.2227\n",
      "Epoch 6 Batch 50 Loss 0.1191 Accuracy 0.2312\n",
      "Epoch 6 Batch 100 Loss 0.1267 Accuracy 0.2318\n",
      "Epoch 6 Batch 150 Loss 0.1397 Accuracy 0.2315\n",
      "Epoch 6 Batch 200 Loss 0.1502 Accuracy 0.2312\n",
      "Epoch 6 Batch 250 Loss 0.1643 Accuracy 0.2311\n",
      "Epoch 6 Batch 300 Loss 0.1823 Accuracy 0.2306\n",
      "Epoch 6 Loss 0.1874 Accuracy 0.2305\n",
      "Time taken for 1 epoch: 161.1419551372528 secs\n",
      "\n",
      "<start> the student handed the examination papers in to the teacher <end>\n",
      "[[1, 3, 435, 2034, 3, 1707, 2267, 11, 5, 3, 242, 2]]\n",
      "Epoch 7 Batch 0 Loss 0.0931 Accuracy 0.2294\n",
      "Epoch 7 Batch 50 Loss 0.1227 Accuracy 0.2326\n",
      "Epoch 7 Batch 100 Loss 0.1281 Accuracy 0.2337\n",
      "Epoch 7 Batch 150 Loss 0.1370 Accuracy 0.2330\n",
      "Epoch 7 Batch 200 Loss 0.1492 Accuracy 0.2320\n",
      "Epoch 7 Batch 250 Loss 0.1648 Accuracy 0.2316\n",
      "Epoch 7 Batch 300 Loss 0.1833 Accuracy 0.2308\n",
      "Epoch 7 Loss 0.1879 Accuracy 0.2306\n",
      "Time taken for 1 epoch: 160.96462321281433 secs\n",
      "\n",
      "<start> he was in good spirits <end>\n",
      "[[1, 9, 21, 11, 57, 4671, 2]]\n",
      "Epoch 8 Batch 0 Loss 0.1180 Accuracy 0.2450\n",
      "Epoch 8 Batch 50 Loss 0.1150 Accuracy 0.2352\n",
      "Epoch 8 Batch 100 Loss 0.1237 Accuracy 0.2335\n",
      "Epoch 8 Batch 150 Loss 0.1351 Accuracy 0.2323\n",
      "Epoch 8 Batch 200 Loss 0.1495 Accuracy 0.2318\n",
      "Epoch 8 Batch 250 Loss 0.1651 Accuracy 0.2312\n",
      "Epoch 8 Batch 300 Loss 0.1850 Accuracy 0.2306\n",
      "Epoch 8 Loss 0.1881 Accuracy 0.2306\n",
      "Time taken for 1 epoch: 162.55679202079773 secs\n",
      "\n",
      "<start> somebodys got to do something <end>\n",
      "[[1, 5422, 78, 5, 19, 128, 2]]\n",
      "Epoch 9 Batch 0 Loss 0.0816 Accuracy 0.2366\n",
      "Epoch 9 Batch 50 Loss 0.1237 Accuracy 0.2312\n",
      "Epoch 9 Batch 100 Loss 0.1295 Accuracy 0.2311\n",
      "Epoch 9 Batch 150 Loss 0.1394 Accuracy 0.2315\n",
      "Epoch 9 Batch 200 Loss 0.1543 Accuracy 0.2316\n",
      "Epoch 9 Batch 250 Loss 0.1689 Accuracy 0.2311\n",
      "Epoch 9 Batch 300 Loss 0.1886 Accuracy 0.2306\n",
      "Epoch 9 Loss 0.1919 Accuracy 0.2304\n",
      "Time taken for 1 epoch: 161.71751999855042 secs\n",
      "\n",
      "<start> in the light of what you told us COMMA i think we should revise our plan <end>\n",
      "[[1, 11, 3, 502, 12, 30, 6, 163, 96, 14, 4, 65, 27, 74, 7327, 117, 254, 2]]\n",
      "Epoch 10 Batch 0 Loss 0.1419 Accuracy 0.2327\n",
      "Epoch 10 Batch 50 Loss 0.1219 Accuracy 0.2327\n",
      "Epoch 10 Batch 100 Loss 0.1266 Accuracy 0.2345\n",
      "Epoch 10 Batch 150 Loss 0.1399 Accuracy 0.2332\n",
      "Epoch 10 Batch 200 Loss 0.1537 Accuracy 0.2327\n",
      "Epoch 10 Batch 250 Loss 0.1650 Accuracy 0.2317\n",
      "Epoch 10 Batch 300 Loss 0.1824 Accuracy 0.2309\n",
      "Saving checkpoint for epoch 10 at data/training_checkpoints/transformer/ckpt-18\n",
      "Epoch 10 Loss 0.1857 Accuracy 0.2307\n",
      "Time taken for 1 epoch: 161.84778809547424 secs\n",
      "\n",
      "<start> how do you pronounce this word? <end>\n",
      "[[1, 47, 19, 6, 2133, 16, 1650, 2]]\n",
      "Epoch 11 Batch 0 Loss 0.1321 Accuracy 0.2444\n",
      "Epoch 11 Batch 50 Loss 0.1173 Accuracy 0.2338\n",
      "Epoch 11 Batch 100 Loss 0.1270 Accuracy 0.2331\n",
      "Epoch 11 Batch 150 Loss 0.1355 Accuracy 0.2332\n",
      "Epoch 11 Batch 200 Loss 0.1478 Accuracy 0.2326\n",
      "Epoch 11 Batch 250 Loss 0.1633 Accuracy 0.2318\n",
      "Epoch 11 Batch 300 Loss 0.1792 Accuracy 0.2314\n",
      "Epoch 11 Loss 0.1838 Accuracy 0.2309\n",
      "Time taken for 1 epoch: 161.44232177734375 secs\n",
      "\n",
      "<start> you wont be late COMMA will you? <end>\n",
      "[[1, 6, 222, 33, 177, 14, 40, 158, 2]]\n",
      "Epoch 12 Batch 0 Loss 0.1115 Accuracy 0.2232\n",
      "Epoch 12 Batch 50 Loss 0.1153 Accuracy 0.2339\n",
      "Epoch 12 Batch 100 Loss 0.1263 Accuracy 0.2327\n",
      "Epoch 12 Batch 150 Loss 0.1368 Accuracy 0.2328\n",
      "Epoch 12 Batch 200 Loss 0.1514 Accuracy 0.2319\n",
      "Epoch 12 Batch 250 Loss 0.1660 Accuracy 0.2310\n",
      "Epoch 12 Batch 300 Loss 0.1826 Accuracy 0.2308\n",
      "Epoch 12 Loss 0.1863 Accuracy 0.2307\n",
      "Time taken for 1 epoch: 161.76747703552246 secs\n",
      "\n",
      "<start> i dont have a computer at home <end>\n",
      "[[1, 4, 24, 18, 7, 705, 31, 99, 2]]\n",
      "Epoch 13 Batch 0 Loss 0.0978 Accuracy 0.2221\n",
      "Epoch 13 Batch 50 Loss 0.1130 Accuracy 0.2344\n",
      "Epoch 13 Batch 100 Loss 0.1252 Accuracy 0.2348\n",
      "Epoch 13 Batch 150 Loss 0.1351 Accuracy 0.2336\n",
      "Epoch 13 Batch 200 Loss 0.1505 Accuracy 0.2327\n",
      "Epoch 13 Batch 250 Loss 0.1663 Accuracy 0.2314\n",
      "Epoch 13 Batch 300 Loss 0.1844 Accuracy 0.2308\n",
      "Epoch 13 Loss 0.1882 Accuracy 0.2306\n",
      "Time taken for 1 epoch: 161.47882318496704 secs\n",
      "\n",
      "<start> hes a friend of my brothers <end>\n",
      "[[1, 120, 7, 288, 12, 13, 820, 2]]\n",
      "Epoch 14 Batch 0 Loss 0.0950 Accuracy 0.2411\n",
      "Epoch 14 Batch 50 Loss 0.1186 Accuracy 0.2338\n",
      "Epoch 14 Batch 100 Loss 0.1231 Accuracy 0.2340\n",
      "Epoch 14 Batch 150 Loss 0.1349 Accuracy 0.2330\n",
      "Epoch 14 Batch 200 Loss 0.1481 Accuracy 0.2324\n",
      "Epoch 14 Batch 250 Loss 0.1632 Accuracy 0.2317\n",
      "Epoch 14 Batch 300 Loss 0.1814 Accuracy 0.2309\n",
      "Epoch 14 Loss 0.1852 Accuracy 0.2307\n",
      "Time taken for 1 epoch: 162.42369604110718 secs\n",
      "\n",
      "<start> i know what youre doing <end>\n",
      "[[1, 4, 41, 30, 111, 245, 2]]\n",
      "Epoch 15 Batch 0 Loss 0.0961 Accuracy 0.2305\n",
      "Epoch 15 Batch 50 Loss 0.1120 Accuracy 0.2342\n",
      "Epoch 15 Batch 100 Loss 0.1237 Accuracy 0.2339\n",
      "Epoch 15 Batch 150 Loss 0.1319 Accuracy 0.2330\n",
      "Epoch 15 Batch 200 Loss 0.1433 Accuracy 0.2329\n",
      "Epoch 15 Batch 250 Loss 0.1607 Accuracy 0.2317\n",
      "Epoch 15 Batch 300 Loss 0.1812 Accuracy 0.2312\n",
      "Saving checkpoint for epoch 15 at data/training_checkpoints/transformer/ckpt-19\n",
      "Epoch 15 Loss 0.1841 Accuracy 0.2312\n",
      "Time taken for 1 epoch: 162.35753393173218 secs\n",
      "\n",
      "<start> he had no idea why his wife left him <end>\n",
      "[[1, 9, 59, 60, 372, 103, 29, 468, 207, 45, 2]]\n",
      "Epoch 16 Batch 0 Loss 0.1401 Accuracy 0.2321\n",
      "Epoch 16 Batch 50 Loss 0.1164 Accuracy 0.2348\n",
      "Epoch 16 Batch 100 Loss 0.1230 Accuracy 0.2338\n",
      "Epoch 16 Batch 150 Loss 0.1342 Accuracy 0.2329\n",
      "Epoch 16 Batch 200 Loss 0.1480 Accuracy 0.2325\n",
      "Epoch 16 Batch 250 Loss 0.1630 Accuracy 0.2316\n",
      "Epoch 16 Batch 300 Loss 0.1824 Accuracy 0.2310\n",
      "Epoch 16 Loss 0.1856 Accuracy 0.2308\n",
      "Time taken for 1 epoch: 161.53463912010193 secs\n",
      "\n",
      "<start> his speech impressed us very much <end>\n",
      "[[1, 29, 902, 1871, 96, 46, 91, 2]]\n",
      "Epoch 17 Batch 0 Loss 0.1251 Accuracy 0.2433\n",
      "Epoch 17 Batch 50 Loss 0.1112 Accuracy 0.2339\n",
      "Epoch 17 Batch 100 Loss 0.1212 Accuracy 0.2338\n",
      "Epoch 17 Batch 150 Loss 0.1321 Accuracy 0.2321\n",
      "Epoch 17 Batch 200 Loss 0.1483 Accuracy 0.2316\n",
      "Epoch 17 Batch 250 Loss 0.1618 Accuracy 0.2315\n",
      "Epoch 17 Batch 300 Loss 0.1798 Accuracy 0.2311\n",
      "Epoch 17 Loss 0.1839 Accuracy 0.2309\n",
      "Time taken for 1 epoch: 161.7494568824768 secs\n",
      "\n",
      "<start> i need your advice <end>\n",
      "[[1, 4, 105, 25, 486, 2]]\n",
      "Epoch 18 Batch 0 Loss 0.0953 Accuracy 0.2360\n",
      "Epoch 18 Batch 50 Loss 0.1071 Accuracy 0.2346\n",
      "Epoch 18 Batch 100 Loss 0.1182 Accuracy 0.2343\n",
      "Epoch 18 Batch 150 Loss 0.1313 Accuracy 0.2339\n",
      "Epoch 18 Batch 200 Loss 0.1441 Accuracy 0.2333\n",
      "Epoch 18 Batch 250 Loss 0.1596 Accuracy 0.2326\n",
      "Epoch 18 Batch 300 Loss 0.1807 Accuracy 0.2312\n",
      "Epoch 18 Loss 0.1836 Accuracy 0.2310\n",
      "Time taken for 1 epoch: 160.46252536773682 secs\n",
      "\n",
      "<start> i remember seeing her <end>\n",
      "[[1, 4, 292, 623, 37, 2]]\n",
      "Epoch 19 Batch 0 Loss 0.1414 Accuracy 0.2260\n",
      "Epoch 19 Batch 50 Loss 0.1241 Accuracy 0.2321\n",
      "Epoch 19 Batch 100 Loss 0.1297 Accuracy 0.2319\n",
      "Epoch 19 Batch 150 Loss 0.1402 Accuracy 0.2318\n",
      "Epoch 19 Batch 200 Loss 0.1516 Accuracy 0.2324\n",
      "Epoch 19 Batch 250 Loss 0.1660 Accuracy 0.2320\n",
      "Epoch 19 Batch 300 Loss 0.1842 Accuracy 0.2311\n",
      "Epoch 19 Loss 0.1878 Accuracy 0.2309\n",
      "Time taken for 1 epoch: 161.74212789535522 secs\n",
      "\n",
      "<start> can you tell those twins apart? <end>\n",
      "[[1, 38, 6, 122, 358, 2300, 3744, 2]]\n",
      "Epoch 20 Batch 0 Loss 0.1006 Accuracy 0.2433\n",
      "Epoch 20 Batch 50 Loss 0.1149 Accuracy 0.2348\n",
      "Epoch 20 Batch 100 Loss 0.1220 Accuracy 0.2344\n",
      "Epoch 20 Batch 150 Loss 0.1326 Accuracy 0.2337\n",
      "Epoch 20 Batch 200 Loss 0.1461 Accuracy 0.2327\n",
      "Epoch 20 Batch 250 Loss 0.1608 Accuracy 0.2318\n",
      "Epoch 20 Batch 300 Loss 0.1786 Accuracy 0.2313\n",
      "Saving checkpoint for epoch 20 at data/training_checkpoints/transformer/ckpt-20\n",
      "Epoch 20 Loss 0.1814 Accuracy 0.2312\n",
      "Time taken for 1 epoch: 162.303386926651 secs\n",
      "\n",
      "<start> the man you saw yesterday was my uncle <end>\n",
      "[[1, 3, 155, 6, 213, 138, 21, 13, 463, 2]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "    \n",
    "    # try a translation to see where we at\n",
    "    try:\n",
    "        translate(None, plot=True)\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The following steps are used for evaluation:\n",
    "\n",
    "* Encode the input sentence using our chinese tokenizer (`tokenizer_zh`). Moreover, add the start and end sentinel tokens so the input is equivalent to what the model is trained with. This is the encoder input.\n",
    "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
    "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
    "\n",
    "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the original Transformer paper, use the entire dataset and base transformer model or transformer XL, by changing our hyperparameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    #start_token = [tokenizer_pt.vocab_size]\n",
    "    #end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    #inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    #inp_sentence = [targ_lang.word_index['<start>']]\n",
    "    #inp_sentence.extend(inp_sentence)\n",
    "    #inp_sentence.append(targ_lang.word_index['<end>'])\n",
    "    inp_sentence = [targ_lang.word_index['<start>']] + inp_sentence[0] + [targ_lang.word_index['<end>']]\n",
    "    print(inp_sentence)\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    #decoder_input = [tokenizer_en.vocab_size]\n",
    "    decoder_input = [targ_lang.word_index['<start>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang.word_index['<end>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wǒ   xǐhuān   zhōngguó   cài'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinyin_jyutping_sentence\n",
    "pinyin_jyutping_sentence.pinyin(\"我 喜欢 中国 菜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    #sentence = tokenizer_pt.encode(sentence)\n",
    "    sentence = inp_lang.texts_to_sequences([sentence])\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        #ax.set_xticklabels(\n",
    "        #    ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "        #    fontdict=fontdict, rotation=90)\n",
    "        #ax.set_xticklabels(\n",
    "        #    ['<start>'] + [inp_lang.sequences_to_texts([i]) for i in sentence] + ['<end>'], \n",
    "        #    fontdict=fontdict, rotation=90)\n",
    "        xticks = ['<start>']\n",
    "        words = [inp_lang.sequences_to_texts([i]) for i in sentence]\n",
    "        xticks.extend(words[0][0].split(' '))\n",
    "        xticks.append('<end>')\n",
    "        print('xticks:',xticks)\n",
    "        ax.set_xticklabels(\n",
    "            xticks, \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "\n",
    "        print(result.numpy())\n",
    "        twords = [targ_lang.sequences_to_texts([result.numpy()])]\n",
    "        print(twords)\n",
    "        twords = twords[0][0].split(' ')\n",
    "        pwords = pinyin_jyutping_sentence.pinyin(' '.join(twords[1:]))\n",
    "        print(pwords)\n",
    "        yticks = [] #['<start>']\n",
    "        yticks.extend(pwords.split(' '))\n",
    "        yticks = [t for t in yticks if t]\n",
    "        print('yticks:',yticks)\n",
    "        #ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "        #                    if i < tokenizer_en.vocab_size], \n",
    "        #                   fontdict=fontdict)\n",
    "        #ax.set_yticklabels([targ_lang.sequences_to_texts([i]) for i in result \n",
    "        #                    if i < targ_lang.num_words], \n",
    "        #                   fontdict=fontdict)\n",
    "        ax.set_yticklabels(yticks, \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>', 'i', 'will', 'go', 'there', '<end>']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = inp_lang.texts_to_sequences([\"i will go there\"])\n",
    "xticks = ['<start>']\n",
    "words = [inp_lang.sequences_to_texts([i]) for i in sentence]\n",
    "xticks.extend(words[0][0].split(' '))\n",
    "xticks.append('<end>')\n",
    "xticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> what is the ultimate purpose of education? <end>\n",
      "[[1, 30, 8, 3, 6558, 1103, 12, 6559, 2]]\n"
     ]
    }
   ],
   "source": [
    "sentence = en[np.random.choice(len(en))]\n",
    "print(sentence)\n",
    "sentence_seq = inp_lang.texts_to_sequences([sentence])\n",
    "print(sentence_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence=None, plot=''):\n",
    "    if sentence is None:\n",
    "        sentence = en[np.random.choice(len(en))]\n",
    "    print(sentence)\n",
    "    sentence_seq = inp_lang.texts_to_sequences([sentence])\n",
    "    print(sentence_seq)\n",
    "    \n",
    "    result, attention_weights = evaluate(sentence_seq)\n",
    "    print(result.numpy())\n",
    "\n",
    "    #predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "    #                                        if i < tokenizer_en.vocab_size])  \n",
    "    predicted_sentence = targ_lang.sequences_to_texts([result.numpy()])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 16, 8, 7, 230, 27, 18, 5, 2]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[targ_lang.word_index['<start>']] + [[16, 8, 7, 230, 27, 18, 5]][0] + [targ_lang.word_index['<end>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a problem we have to solve.\n",
      "[[16, 8, 7, 230, 27, 18, 5]]\n",
      "[1, 16, 8, 7, 230, 27, 18, 5, 2]\n",
      "[  1 134  24  14  91   4  75]\n",
      "Input: This is a problem we have to solve.\n",
      "Predicted translation: ['<start> 这是 一个 我们 必须 的 问题']\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That translates to `This is a question we must` (Zhè shì yīgè wǒmen bìxū de wèntí).\n",
    "\n",
    "It looks like the last word is not being picked up. I must have an end of sequence bug in my code. Can you fix it?\n",
    "\n",
    "For the time being, I'll just repeat the last word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a problem we have to solve solve.\n",
      "[[16, 8, 7, 230, 27, 18, 5, 803]]\n",
      "[1, 16, 8, 7, 230, 27, 18, 5, 803, 2]\n",
      "[  1 134  24  14  91 348   4  75]\n",
      "Input: This is a problem we have to solve solve.\n",
      "Predicted translation: ['<start> 这是 一个 我们 必须 解决 的 问题']\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a problem we have to solve solve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That translates correcly! `This is a problem we must solve`, `这是 一个 我们 必须 解决 的 问题` (Zhè shì yīgè wǒmen bìxū jiějué de wèntí)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like watching tv after dinner dinner.\n",
      "[[4, 32, 480, 356, 162, 379]]\n",
      "[1, 4, 32, 480, 356, 162, 379, 2]\n",
      "[  1   3  26   8 567 418]\n",
      "Input: I like watching tv after dinner dinner.\n",
      "Predicted translation: ['<start> 我 喜欢 在 晚餐 看电视']\n"
     ]
    }
   ],
   "source": [
    "translate(\"I like watching tv after dinner dinner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That translates to `I like watching TV at dinner` (Wǒ xǐhuān zài wǎncān kàn diànshì)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like watching tv after i wake up wake up.\n",
      "[[4, 32, 480, 356, 162, 4, 744, 56, 744]]\n",
      "[1, 4, 32, 480, 356, 162, 4, 744, 56, 744, 2]\n",
      "[  1   3  26   8 418 154 921]\n",
      "Input: I like watching tv after i wake up wake up.\n",
      "Predicted translation: ['<start> 我 喜欢 在 看电视 后 洗澡']\n"
     ]
    }
   ],
   "source": [
    "translate(\"I like watching tv after i wake up wake up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That translates to `I like to take a shower after watching TV` (Wǒ xǐhuān zài kàn diànshì hòu xǐzǎo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will go running tomorrow night.\n",
      "[[4, 40, 39, 673, 129]]\n",
      "[1, 4, 40, 39, 673, 129, 2]\n",
      "[  1   3  83  38  19 395]\n",
      "Input: I will go running tomorrow night.\n",
      "Predicted translation: ['<start> 我 明天 要 去 散步']\n",
      "Real translation: 我 明天 晚上 要 去 散步.\n"
     ]
    }
   ],
   "source": [
    "translate(\"I will go running tomorrow night.\")\n",
    "print (\"Real translation: 我 明天 晚上 要 去 散步.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass different layers and attention blocks of the decoder to the `plot` parameter in order to plot attention heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will go running tomorrow night.\n",
      "[[4, 40, 39, 673, 129]]\n",
      "[1, 4, 40, 39, 673, 129, 2]\n",
      "[  1   3  83  38  19 395]\n",
      "Input: I will go running tomorrow night.\n",
      "Predicted translation: ['<start> 我 明天 要 去 散步']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "xticks: ['<start>', 'i', 'will', 'go', 'running', 'tomorrow', '<end>']\n",
      "[  1   3  83  38  19 395]\n",
      "[['<start> 我 明天 要 去 散步']]\n",
      "wǒ   míngtiān   yào   qù   sànbù\n",
      "yticks: ['wǒ', 'míngtiān', 'yào', 'qù', 'sànbù']\n",
      "Real translation: 我 明天 晚上 要 去 散步.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1779e3e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"I will go running tomorrow night.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: 我 明天 晚上 要 去 散步.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save_weights(\"data/training_checkpoints/transformer/transformer100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We learned how to create a **transformer**, the state-of-the-art-model in NLP! \n",
    "\n",
    "After 100 epochs, which were also faster than seq2seq with attention, the translations are markedly better than seq2seq with attention.\n",
    "\n",
    "# Research/Homework\n",
    "- Train the model from scratch for 100 epochs and compare with seq2seq with attention. No GPU for now.\n",
    "- If you're not able to, next week I will give you my saved weights to restore and translate sentences.\n",
    "- Fix my last-word-of-the-sentence bug\n",
    "- Try using a different dataset to train the transformer\n",
    "- Create the full base transformer or transformer XL by changing the hyperparameters above, and maybe train for more epochs\n",
    "- Use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models\n",
    "- Implement **beam search** to get better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: TFDS processing\n",
    "\n",
    "Many of the datasets that researchers work on in NLP can be found [here](https://www.tensorflow.org/datasets/catalog/overview). However, these datasets are in a special format that requires its own \"*special processing*\". Here it is. I modified it to work with the 1.x version of tf, but will probably work for 2.x, too.\n",
    "```(python)\n",
    "pip install tensorflow_datasets\n",
    "```\n",
    "\n",
    "You should run the cell below at the very beginning of your notebook. Also note that eager execution is not required for tf 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This errored out for me, but it informs you which datasets are available as a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(python)\n",
    "DatasetNotFoundError: Dataset ted_hrlr_translate not found. Available datasets:\n",
    "\t- bair_robot_pushing_small\n",
    "\t- cats_vs_dogs\n",
    "\t- celeb_a\n",
    "\t- celeb_a_hq\n",
    "\t- cifar10\n",
    "\t- cifar100\n",
    "\t- coco2014\n",
    "\t- diabetic_retinopathy_detection\n",
    "\t- dummy_dataset_shared_generator\n",
    "\t- dummy_mnist\n",
    "\t- fashion_mnist\n",
    "\t- image_label_folder\n",
    "\t- imagenet2012\n",
    "\t- imdb_reviews\n",
    "\t- lm1b\n",
    "\t- lsun\n",
    "\t- mnist\n",
    "\t- moving_mnist\n",
    "\t- nsynth\n",
    "\t- omniglot\n",
    "\t- open_images_v4\n",
    "\t- quickdraw_bitmap\n",
    "\t- squad\n",
    "\t- starcraft_video\n",
    "\t- svhn_cropped\n",
    "\t- tf_flowers\n",
    "\t- wmt_translate_ende\n",
    "\t- wmt_translate_enfr\n",
    "Check that:\n",
    "    - the dataset name is spelled correctly\n",
    "    - dataset class defines all base class abstract methods\n",
    "    - dataset class is not in development, i.e. if IN_DEVELOPMENT=True\n",
    "    - the module defining the dataset class is imported\n",
    "```\n",
    "And the error dump allowed me to find and download the english-to-french parallel dataset. This will take about 20 minutes to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('wmt_translate_enfr', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom subwords tokenizer from the training dataset, using `tfds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary. That's a bit strange, but it is the state of the art of tokenization in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(python)\n",
    "5299 ----> Tra\n",
    "629 ----> ns\n",
    "2152 ----> forme\n",
    "42 ----> r \n",
    "484 ----> is \n",
    "7991 ----> a\n",
    "1507 ----> we\n",
    "1248 ----> so\n",
    "460 ----> me\n",
    "7940 ----> .\n",
    "```\n",
    "\n",
    "Buffer and batch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a start and end token to the input and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
    "\n",
    "* Graph tensors do not have a value. \n",
    "* In graph mode you can only use TensorFlow Ops and functions. \n",
    "\n",
    "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To keep this example small and relatively fast, drop examples with a length of over 40 tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to tell you the shape of our training dataset, we'll need that for a named paramter a few cells down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(python)\n",
    "<DatasetV1Adapter shapes: ((?,), (?,)), types: (tf.int64, tf.int64)>\n",
    "```\n",
    "\n",
    "Below is the original google notebook code, but it does not work because it's missing the required named parameter `padded_shapes`. I read on a blog that google fixed this in tf 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this instead:\n",
    "\n",
    "(I got the `((None,),(None,))` from `((?,), (?,))` above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes = ((None,),(None,)))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE, padded_shapes = ((None,),(None,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this should output batches of tokenized sentences in both languages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(python)\n",
    "(<tf.Tensor: id=6056420, shape=(64, 39), dtype=int64, numpy=\n",
    " array([[8215,  186,   50, ...,    0,    0,    0],\n",
    "        [8215,  313,   69, ...,    0,    0,    0],\n",
    "        [8215,  181,   17, ...,    0,    0,    0],\n",
    "        ...,\n",
    "        [8215, 5623, 7991, ...,    0,    0,    0],\n",
    "        [8215,   99,  201, ...,    0,    0,    0],\n",
    "        [8215,   68,   97, ...,    0,    0,    0]], dtype=int64)>,\n",
    " <tf.Tensor: id=6056421, shape=(64, 40), dtype=int64, numpy=\n",
    " array([[8150,  963, 1734, ...,    0,    0,    0],\n",
    "        [8150,  149,   11, ...,    0,    0,    0],\n",
    "        [8150,  797,   57, ...,    0,    0,    0],\n",
    "        ...,\n",
    "        [8150, 7959, 1453, ...,    0,    0,    0],\n",
    "        [8150,  102,    3, ...,    0,    0,    0],\n",
    "        [8150,   77,    7, ...,    0,    0,    0]], dtype=int64)>)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    print(inp, tar)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(python)\n",
    "tf.Tensor(\n",
    "[[8215   62 5941 ...    0    0    0]\n",
    " [8215   46   32 ...    0    0    0]\n",
    " [8215 3472  377 ...    0    0    0]\n",
    " ...\n",
    " [8215  150   24 ...    0    0    0]\n",
    " [8215   35   43 ...    0    0    0]\n",
    " [8215   11  105 ...    0    0    0]], shape=(64, 36), dtype=int64) tf.Tensor(\n",
    "[[8150  373  609 ...    0    0    0]\n",
    " [8150  178 8004 ...    0    0    0]\n",
    " [8150 1800    7 ...    0    0    0]\n",
    " ...\n",
    " [8150   44 7337 ...    0    0    0]\n",
    " [8150   44  646 ...    0    0    0]\n",
    " [8150   64    5 ...    0    0    0]], shape=(64, 40), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest should be pretty similar to our own dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
